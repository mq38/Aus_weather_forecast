{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70384ab",
   "metadata": {},
   "source": [
    "# Predicting weather in Australia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9065ed-2e79-4f5a-97a3-cfbd22afe1d9",
   "metadata": {},
   "source": [
    "Made by Mihkel Paal and Laura Heleene Tirkkonen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d3b14",
   "metadata": {},
   "source": [
    "## 1. Importing libraries/data and encoding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9c86634d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
      "0           W           44.0          W  ...        71.0         22.0   \n",
      "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
      "2         WSW           46.0          W  ...        38.0         30.0   \n",
      "3          NE           24.0         SE  ...        45.0         16.0   \n",
      "4           W           41.0        ENE  ...        82.0         33.0   \n",
      "\n",
      "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
      "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
      "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
      "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
      "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
      "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
      "\n",
      "   RainTomorrow  \n",
      "0            No  \n",
      "1            No  \n",
      "2            No  \n",
      "3            No  \n",
      "4            No  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "There are 7 categorical variables\n",
      "\n",
      "Categorical variables are : ['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import geopandas as gpd    \n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # data visualization\n",
    "sns.reset_defaults()\n",
    "import geoplot as gplt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.interpolate import griddata\n",
    "#read data\n",
    "\n",
    "df = pd.read_csv(\"weatherAUS.csv\")\n",
    "print(df.head())\n",
    "#df.info()\n",
    "\n",
    "df_map = pd.read_csv(\"weatherAUS.csv\")\n",
    "\n",
    "\n",
    "## Find categorical variables\n",
    "\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('Categorical variables are :', categorical)\n",
    "\n",
    "## find missing values in categorical variables\n",
    "\n",
    "#print(df[categorical].isnull().sum())\n",
    "\n",
    "##frequency of categorical variables\n",
    "\n",
    "#for var in categorical: \n",
    "        \n",
    "#print(df[var].value_counts())\n",
    "\n",
    "##check for cardinality in categorical variables\n",
    "\n",
    "#for var in categorical:\n",
    "    \n",
    "#    print(var, ' contains ', len(df[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "17dafde9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## date variable contains 3436 labels so needs to be split into year/month/day\n",
    "\n",
    "#print(df[\"Date\"].dtypes)\n",
    "\n",
    "df['Date']= pd.to_datetime(df['Date'])\n",
    "\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "df.drop('Date', axis=1, inplace = True)\n",
    "\n",
    "#start looking into other categorical variables\n",
    "\n",
    "#print('Location contains', len(df.Location.unique()), 'labels')\n",
    "\n",
    "#print(df.Location.unique())\n",
    "\n",
    "#one-hot encoding for categorical variables\n",
    "\n",
    "# add most popular values for missing categorical values\n",
    "\n",
    "for df2 in [df]:\n",
    "    df2['WindGustDir'] = df2['WindGustDir'].fillna(df2['WindGustDir'].mode()[0])\n",
    "    df2['WindDir9am'] = df2['WindDir9am'].fillna(df2['WindDir9am'].mode()[0])\n",
    "    df2['WindDir3pm'] = df2['WindDir3pm'].fillna(df2['WindDir3pm'].mode()[0])\n",
    "    df2['RainToday'] = df2['RainToday'].fillna(df2['RainToday'].mode()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ca6d5e79-e107-436b-ab4c-5c3a96deed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## location is necessary for maps, so this is omitted as a dummy variable from df_map\n",
    "df_map = pd.get_dummies(df, columns=[ 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'], drop_first=True, dummy_na=True)\n",
    "\n",
    "## for other needs\n",
    "df = pd.get_dummies(df, columns=['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'], drop_first=True, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a541ea72-8513-48a3-bde0-ca36846bbada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTemp            1485\n",
      "MaxTemp            1261\n",
      "Rainfall           3261\n",
      "Evaporation       62790\n",
      "Sunshine          69835\n",
      "                  ...  \n",
      "WindDir3pm_WNW        0\n",
      "WindDir3pm_WSW        0\n",
      "WindDir3pm_nan        0\n",
      "RainToday_Yes         0\n",
      "RainToday_nan         0\n",
      "Length: 118, dtype: int64\n",
      "        MinTemp   MaxTemp  Rainfall  Evaporation  Sunshine  WindGustSpeed  \\\n",
      "count  143975.0  144199.0  142199.0      82670.0   75625.0       135197.0   \n",
      "mean       12.0      23.0       2.0          5.0       8.0           40.0   \n",
      "std         6.0       7.0       8.0          4.0       4.0           14.0   \n",
      "min        -8.0      -5.0       0.0          0.0       0.0            6.0   \n",
      "25%         8.0      18.0       0.0          3.0       5.0           31.0   \n",
      "50%        12.0      23.0       0.0          5.0       8.0           39.0   \n",
      "75%        17.0      28.0       1.0          7.0      11.0           48.0   \n",
      "max        34.0      48.0     371.0        145.0      14.0          135.0   \n",
      "\n",
      "       WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  Pressure9am  \\\n",
      "count      143693.0      142398.0     142806.0     140953.0     130395.0   \n",
      "mean           14.0          19.0         69.0         52.0       1018.0   \n",
      "std             9.0           9.0         19.0         21.0          7.0   \n",
      "min             0.0           0.0          0.0          0.0        980.0   \n",
      "25%             7.0          13.0         57.0         37.0       1013.0   \n",
      "50%            13.0          19.0         70.0         52.0       1018.0   \n",
      "75%            19.0          24.0         83.0         66.0       1022.0   \n",
      "max           130.0          87.0        100.0        100.0       1041.0   \n",
      "\n",
      "       Pressure3pm  Cloud9am  Cloud3pm   Temp9am   Temp3pm      Year  \\\n",
      "count     130432.0   89572.0   86102.0  143693.0  141851.0  145460.0   \n",
      "mean        1015.0       4.0       5.0      17.0      22.0    2013.0   \n",
      "std            7.0       3.0       3.0       6.0       7.0       3.0   \n",
      "min          977.0       0.0       0.0      -7.0      -5.0    2007.0   \n",
      "25%         1010.0       1.0       2.0      12.0      17.0    2011.0   \n",
      "50%         1015.0       5.0       5.0      17.0      21.0    2013.0   \n",
      "75%         1020.0       7.0       7.0      22.0      26.0    2015.0   \n",
      "max         1040.0       9.0       9.0      40.0      47.0    2017.0   \n",
      "\n",
      "          Month       Day  \n",
      "count  145460.0  145460.0  \n",
      "mean        6.0      16.0  \n",
      "std         3.0       9.0  \n",
      "min         1.0       1.0  \n",
      "25%         3.0       8.0  \n",
      "50%         6.0      16.0  \n",
      "75%         9.0      23.0  \n",
      "max        12.0      31.0   2\n"
     ]
    }
   ],
   "source": [
    "# explore numerical variables\n",
    "\n",
    "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "\n",
    "#print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "#print('The numerical variables are :', numerical)\n",
    "\n",
    "#19 numerical variables, all continuous type\n",
    "#check for missing values\n",
    "\n",
    "print(df[numerical].isnull().sum())\n",
    "\n",
    "print(round(df[numerical].describe()),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e8cb1",
   "metadata": {},
   "source": [
    "## 2. Plotting weather variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b081edfd-1a98-4a63-8b62-6e164469ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plots\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = df.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = df.boxplot(column='Evaporation')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Evaporation')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "fig = df.boxplot(column='WindSpeed9am')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed9am')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "fig = df.boxplot(column='WindSpeed3pm')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed3pm')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ad039",
   "metadata": {},
   "source": [
    "## 3. Prediction of rain tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bcfa8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting column 'RainTomorrow' to numeric variable\n",
    "\n",
    "df['RainTomorrow'] = df['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "## handling missing values in column 'RainTomorrow'\n",
    "df = df.dropna(subset=['RainTomorrow'])  # dropping rows with NaN in 'RainTomorrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5b248c66-c568-4044-a145-7b24ef654327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating dataframes with and without target \n",
    "\n",
    "X = df.drop(['RainTomorrow'], axis=1)\n",
    "\n",
    "y = df['RainTomorrow']\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## replacing missing values with mean for imputation\n",
    "\n",
    "num_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "num_imputer = SimpleImputer(strategy='mean')  # replacing NaNs with mean for numerical features\n",
    "X[num_cols] = num_imputer.fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a4e343a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## splitting data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "908613bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7963711804212525\n",
      "[[17632  4380]\n",
      " [ 1411  5016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.80      0.86     22012\n",
      "         1.0       0.53      0.78      0.63      6427\n",
      "\n",
      "    accuracy                           0.80     28439\n",
      "   macro avg       0.73      0.79      0.75     28439\n",
      "weighted avg       0.84      0.80      0.81     28439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## scaling data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## initializing logistic regression model --> as we have binary classification task\n",
    "## logistic regression assumes linear relationship between variables\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model_logreg = LogisticRegression(class_weight='balanced', C=0.1, solver='lbfgs')\n",
    "model_logreg.fit(X_train, y_train)\n",
    "\n",
    "## predictions and evaluation\n",
    "y_pred = model_logreg.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "54de1ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "## finding the best parameters for logistic regression model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcc108",
   "metadata": {},
   "source": [
    "## 4. Creating maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96eda74",
   "metadata": {},
   "source": [
    "### 4.1 Example temperature map for April 30th, 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "77c4351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map of temperatures on a certain time\n",
    "## must use df_map\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# filter data for the specific date\n",
    "df_filtered = df_map[(df_map['Year'] == 2012) & (df_map['Month'] == 4) & (df_map['Day'] == 30)].copy()\n",
    "\n",
    "# geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# Create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in df_filtered['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# 3. Add latitude and longitude to the filtered dataframe\n",
    "df_filtered.loc[:, 'Longitude'] = lons\n",
    "df_filtered.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# 4. Drop rows with missing coordinates or temperature data\n",
    "df_filtered = df_filtered.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# 5. Extract columns for plotting\n",
    "lons = df_filtered['Longitude']\n",
    "lats = df_filtered['Latitude']\n",
    "temps = df_filtered['MaxTemp']\n",
    "\n",
    "# 6. Create the map plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# Plot temperature points\n",
    "sc = ax.scatter(lons, lats, c=temps, cmap='coolwarm', edgecolors='k', s=100)\n",
    "\n",
    "# Add temperature labels\n",
    "for idx, row in df_filtered.iterrows():\n",
    "    ax.text(row['Longitude'], row['Latitude'] + 0.2, f\"{row['MaxTemp']}°C\", fontsize=10, ha='center', color='black')  # Temp above the point\n",
    "    ax.text(row['Longitude'], row['Latitude'] - 0.2, row['Location'], fontsize=9, ha='center', color='blue')         # Location below the point\n",
    "\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax, orientation='vertical', label='Max Temperature (°C)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Max Temperature across Australia on 30 April 2012\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38661dc1",
   "metadata": {},
   "source": [
    "### 4.2 Average max temp for all weather stations for january "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2baab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pykrige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d11e2aad-175f-4962-8893-413937b26327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## next map is better for the poster\n",
    "\n",
    "## average max temp for all weather stations for january \n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "krige = OrdinaryKriging(lons, lats, temps, variogram_model='linear')\n",
    "\n",
    "# filter the dataset for January\n",
    "df_january = df_map[df_map['Month'] == 1].copy()\n",
    "\n",
    "# drop rows with missing temperature data\n",
    "df_january = df_january.dropna(subset=['MaxTemp'])\n",
    "\n",
    "# group by weather station and calculate the average temperature\n",
    "# Assuming 'Location' represents the weather station\n",
    "avg_temps_january = df_january.groupby('Location')['MaxTemp'].mean().reset_index()\n",
    "\n",
    "# rename columns for clarity\n",
    "avg_temps_january.columns = ['Location', 'MaxTemp']\n",
    "\n",
    "# display the result\n",
    "##print(avg_temps_january)\n",
    "\n",
    "\n",
    "# geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in avg_temps_january['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# add latitude and longitude to the filtered dataframe using .loc\n",
    "avg_temps_january.loc[:, 'Longitude'] = lons\n",
    "avg_temps_january.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# drop rows with NaN coordinates or missing MaxTemp values\n",
    "avg_temps_january = avg_temps_january.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# extract data for interpolation\n",
    "lons = avg_temps_january['Longitude'].values\n",
    "lats = avg_temps_january['Latitude'].values\n",
    "temps = avg_temps_january['MaxTemp'].values\n",
    "\n",
    "# define a grid for interpolation\n",
    "lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "lon_grid, lat_grid = np.meshgrid(\n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "# interpolate temperature values onto the grid\n",
    "temp_grid, _ = krige.execute(\n",
    "    'grid', \n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# plot interpolated temperature as a contour map\n",
    "contour = ax.contourf(\n",
    "    lon_grid, lat_grid, temp_grid, \n",
    "    levels=20, cmap='coolwarm', transform=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "sc = ax.scatter(\n",
    "    lons, lats, color='black', edgecolors='k', s=100, label='Data Points'\n",
    ")\n",
    "\n",
    "# add location labels\n",
    "for idx, row in avg_temps_january.iterrows():\n",
    "    ax.text(\n",
    "        row['Longitude'], row['Latitude'] + 0.5,  # Slightly offset above each point\n",
    "        row['Location'], fontsize=9, ha='center', color='green'\n",
    "    )\n",
    "\n",
    "\n",
    "# add labels and title\n",
    "plt.title(\"Interpolated average max. temperature across Australia in January\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6242de3",
   "metadata": {},
   "source": [
    "### 4.3 Interpolation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3440d797-2ae9-4a61-9c48-498240564198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes on the poster\n",
    "# Interpolation, uses kriging and looks good\n",
    "# use df_map\n",
    "# pip install pykrige\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "krige = OrdinaryKriging(lons, lats, temps, variogram_model='linear')\n",
    "\n",
    "\n",
    "# 1. Filter data for a specific date\n",
    "df_filtered = df_map[(df_map['Year'] == 2015) & (df_map['Month'] == 7) & (df_map['Day'] == 8)].copy()\n",
    "\n",
    "# 2. Geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# Create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in df_filtered['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the filtered dataframe using .loc\n",
    "df_filtered.loc[:, 'Longitude'] = lons\n",
    "df_filtered.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# Drop rows with NaN coordinates or missing MaxTemp values\n",
    "df_filtered = df_filtered.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract data for interpolation\n",
    "lons = df_filtered['Longitude'].values\n",
    "lats = df_filtered['Latitude'].values\n",
    "temps = df_filtered['MaxTemp'].values\n",
    "\n",
    "# 3. Define a grid for interpolation\n",
    "lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "lon_grid, lat_grid = np.meshgrid(\n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "# 4. Interpolate temperature values onto the grid\n",
    "temp_grid, _ = krige.execute(\n",
    "    'grid', \n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# Plot interpolated temperature as a contour map\n",
    "contour = ax.contourf(\n",
    "    lon_grid, lat_grid, temp_grid, \n",
    "    levels=20, cmap='coolwarm', transform=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "# Add the original data points\n",
    "sc = ax.scatter(lons, lats, edgecolors='k', s=100, label='Location')\n",
    "\n",
    "# Add a colorbar for the temperature scale\n",
    "cbar = plt.colorbar(contour, ax=ax, orientation='vertical', label='Max Temperature (°C)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Interpolated Max Temperature across Australia \")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "185c6d90-e45e-44df-bfab-55b03e1bb002",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Year', 'Month', 'Day'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[242], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# merge year month day back together\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_map[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Drop Year, Month, Day columns if not needed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_map \u001b[38;5;241m=\u001b[39m df_map\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Year', 'Month', 'Day'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# merge year month day back together\n",
    "df_map['Date'] = pd.to_datetime(df_map[['Year', 'Month', 'Day']])\n",
    "\n",
    "# Drop Year, Month, Day columns if not needed\n",
    "df_map = df_map.drop(['Year', 'Month', 'Day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "deadbaa3-c3ad-4284-ac58-e777eb8543a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df_map.select_dtypes(include=['float', 'int']).columns\n",
    "num_imputer = SimpleImputer(strategy='mean')  # Replace NaNs with mean for numerical features\n",
    "df_map[numerical_cols] = num_imputer.fit_transform(df_map[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "83085cc5-5738-4d9b-8e11-5018dacc9dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2008-12-01\n",
       "1        2008-12-02\n",
       "2        2008-12-03\n",
       "3        2008-12-04\n",
       "4        2008-12-05\n",
       "            ...    \n",
       "145455   2017-06-21\n",
       "145456   2017-06-22\n",
       "145457   2017-06-23\n",
       "145458   2017-06-24\n",
       "145459   2017-06-25\n",
       "Name: Date, Length: 145460, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "04e7ad95-36ce-48c2-aeed-ed234a6ca1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.00000000e-01, 0.00000000e+00, 1.00000000e+00, 2.00000000e-01,\n",
       "       1.40000000e+00, 2.20000000e+00, 1.56000000e+01, 3.60000000e+00,\n",
       "       2.36091815e+00, 1.68000000e+01, 1.06000000e+01, 1.20000000e+00,\n",
       "       8.00000000e-01, 6.40000000e+00, 4.00000000e-01, 3.00000000e+00,\n",
       "       5.80000000e+00, 1.16000000e+01, 1.80000000e+00, 8.60000000e+00,\n",
       "       1.26000000e+01, 8.40000000e+00, 6.20000000e+00, 2.00000000e+01,\n",
       "       2.10000000e+01, 3.20000000e+00, 4.80000000e+00, 4.20000000e+00,\n",
       "       8.00000000e+00, 1.44000000e+01, 4.60000000e+00, 2.00000000e+00,\n",
       "       5.60000000e+00, 1.60000000e+00, 6.00000000e+00, 4.40000000e+00,\n",
       "       5.40000000e+00, 5.00000000e+00, 7.80000000e+00, 6.80000000e+00,\n",
       "       9.80000000e+00, 3.80000000e+00, 2.40000000e+00, 5.20000000e+00,\n",
       "       6.60000000e+00, 1.34000000e+01, 1.02000000e+01, 2.88000000e+01,\n",
       "       7.00000000e+00, 2.58000000e+01, 9.40000000e+00, 1.24000000e+01,\n",
       "       5.22000000e+01, 2.06000000e+01, 6.60000000e+01, 1.10000000e+01,\n",
       "       1.70000000e+01, 1.04000000e+01, 2.80000000e+01, 2.14000000e+01,\n",
       "       1.86000000e+01, 7.20000000e+00, 8.20000000e+00, 1.08000000e+01,\n",
       "       1.54000000e+01, 9.60000000e+00, 4.00000000e+00, 2.18000000e+01,\n",
       "       2.08000000e+01, 2.48000000e+01, 1.00000000e+01, 1.14000000e+01,\n",
       "       1.90000000e+01, 2.22000000e+01, 3.28000000e+01, 3.40000000e+00,\n",
       "       5.08000000e+01, 5.26000000e+01, 1.92000000e+01, 1.88000000e+01,\n",
       "       1.48000000e+01, 1.20000000e+01, 1.84000000e+01, 2.56000000e+01,\n",
       "       3.50000000e+01, 1.42000000e+01, 1.38000000e+01, 2.60000000e+00,\n",
       "       9.92000000e+01, 5.10000000e+01, 3.98000000e+01, 2.82000000e+01,\n",
       "       1.22000000e+01, 1.28000000e+01, 1.18000000e+01, 1.98000000e+01,\n",
       "       1.74000000e+01, 8.80000000e+00, 1.62000000e+01, 1.46000000e+01,\n",
       "       2.80000000e+00, 3.08000000e+01, 2.84000000e+01, 3.48000000e+01,\n",
       "       4.40000000e+01, 1.40000000e+01, 1.66000000e+01, 9.00000000e+00,\n",
       "       2.68000000e+01, 3.84000000e+01, 1.04200000e+02, 3.66000000e+01,\n",
       "       3.42000000e+01, 2.94000000e+01, 2.72000000e+01, 1.12000000e+01,\n",
       "       1.32000000e+01, 3.34000000e+01, 7.60000000e+00, 2.46000000e+01,\n",
       "       7.40000000e+00, 1.60000000e+01, 1.94000000e+01, 5.34000000e+01,\n",
       "       1.50000000e+01, 3.36000000e+01, 1.72000000e+01, 3.74000000e+01,\n",
       "       1.30000000e+01, 3.10000000e+01, 9.20000000e+00, 6.62000000e+01,\n",
       "       3.64000000e+01, 4.10000000e+01, 2.44000000e+01, 2.70000000e+01,\n",
       "       4.68000000e+01, 1.58000000e+01, 3.58000000e+01, 2.24000000e+01,\n",
       "       1.64000000e+01, 1.52000000e+01, 2.30000000e+01, 2.28000000e+01,\n",
       "       2.86000000e+01, 4.60000000e+01, 2.42000000e+01, 2.20000000e+01,\n",
       "       1.80000000e+01, 3.12000000e+01, 2.12000000e+01, 3.60000000e+01,\n",
       "       2.34000000e+01, 3.20000000e+01, 4.64000000e+01, 1.36000000e+01,\n",
       "       4.20000000e+01, 3.38000000e+01, 2.54000000e+01, 2.04000000e+01,\n",
       "       2.26000000e+01, 4.32000000e+01, 6.14000000e+01, 4.82000000e+01,\n",
       "       4.86000000e+01, 1.82000000e+01, 6.78000000e+01, 3.92000000e+01,\n",
       "       3.18000000e+01, 8.24000000e+01, 3.54000000e+01, 1.16000000e+02,\n",
       "       3.06000000e+01, 1.78000000e+01, 3.82000000e+01, 3.02000000e+01,\n",
       "       2.40000000e+01, 4.70000000e+01, 4.24000000e+01, 3.30000000e+01,\n",
       "       4.00000000e+01, 4.42000000e+01, 3.46000000e+01, 3.04000000e+01,\n",
       "       3.88000000e+01, 8.36000000e+01, 5.02000000e+01, 2.62000000e+01,\n",
       "       3.80000000e+01, 3.16000000e+01, 2.78000000e+01, 2.98000000e+01,\n",
       "       1.76000000e+01, 1.00200000e+02, 1.09200000e+02, 2.36000000e+01,\n",
       "       3.26000000e+01, 2.16000000e+01, 5.40000000e+01, 3.96000000e+01,\n",
       "       2.60000000e+01, 3.40000000e+01, 4.94000000e+01, 3.70000000e+01,\n",
       "       2.92000000e+01, 6.80000000e+01, 3.32000000e+01, 2.74000000e+01,\n",
       "       3.90000000e+01, 3.86000000e+01, 4.30000000e+01, 4.50000000e+00,\n",
       "       2.91000000e+01, 2.90000000e+00, 1.30000000e+00, 3.00000000e+01,\n",
       "       6.36000000e+01, 1.89000000e+02, 5.86000000e+01, 6.26000000e+01,\n",
       "       5.90000000e+01, 2.02000000e+01, 4.28000000e+01, 7.60000000e+01,\n",
       "       1.96000000e+01, 5.58000000e+01, 7.00000000e+01, 7.12000000e+01,\n",
       "       1.41800000e+02, 9.60000000e+01, 3.71000000e+02, 5.80000000e+01,\n",
       "       6.70000000e+01, 8.06000000e+01, 5.66000000e+01, 4.52000000e+01,\n",
       "       5.94000000e+01, 4.26000000e+01, 3.44000000e+01, 9.00000000e+01,\n",
       "       1.28000000e+02, 6.12000000e+01, 2.38000000e+01, 2.76000000e+01,\n",
       "       7.38000000e+01, 7.22000000e+01, 1.05400000e+02, 4.04000000e+01,\n",
       "       1.02000000e+02, 4.44000000e+01, 4.80000000e+01, 6.38000000e+01,\n",
       "       3.68000000e+01, 5.50000000e+00, 2.52000000e+01, 1.03400000e+02,\n",
       "       1.27600000e+02, 6.32000000e+01, 5.24000000e+01, 2.50000000e+01,\n",
       "       5.50000000e+01, 5.68000000e+01, 7.08000000e+01, 6.24000000e+01,\n",
       "       1.12000000e+02, 1.53200000e+02, 4.58000000e+01, 9.70000000e+01,\n",
       "       5.18000000e+01, 8.64000000e+01, 5.00000000e-01, 1.59800000e+02,\n",
       "       2.08500000e+02, 1.72200000e+02, 9.00000000e-01, 2.50000000e+00,\n",
       "       3.00000000e-01, 1.70000000e+00, 3.30000000e+00, 5.12000000e+01,\n",
       "       1.06400000e+02, 1.47800000e+02, 4.56000000e+01, 5.96000000e+01,\n",
       "       2.90000000e+01, 1.17000000e+01, 7.50000000e+01, 7.64000000e+01,\n",
       "       6.40000000e+01, 5.32000000e+01, 1.00400000e+02, 1.00000000e-01,\n",
       "       6.58000000e+01, 8.50000000e+00, 3.24000000e+01, 1.57800000e+02,\n",
       "       4.10000000e+00, 1.13800000e+02, 7.00000000e-01, 3.10000000e+00,\n",
       "       1.50000000e+00, 3.22000000e+01, 3.27000000e+01, 1.01000000e+01,\n",
       "       1.11000000e+01, 9.61000000e+01, 4.98000000e+01, 2.19600000e+02,\n",
       "       1.09400000e+02, 3.50000000e+00, 2.64000000e+01, 2.96000000e+01,\n",
       "       3.62000000e+01, 3.76000000e+01, 4.38000000e+01, 4.15000000e+01,\n",
       "       5.06000000e+01, 1.42800000e+02, 1.06200000e+02, 1.04800000e+02,\n",
       "       2.70000000e+00, 3.11000000e+01, 1.39000000e+01, 7.10000000e+01,\n",
       "       9.06000000e+01, 6.48000000e+01, 4.76000000e+01, 5.14000000e+01,\n",
       "       2.66000000e+01, 4.88000000e+01, 3.72000000e+01, 3.94000000e+01,\n",
       "       1.13000000e+02, 7.44000000e+01, 9.28000000e+01, 4.02000000e+01,\n",
       "       7.68000000e+01, 5.46000000e+01, 4.48000000e+01, 4.46000000e+01,\n",
       "       5.00000000e+01, 2.27000000e+01, 2.09000000e+01, 5.98000000e+01,\n",
       "       4.54000000e+01, 1.47200000e+02, 3.52000000e+01, 7.14000000e+01,\n",
       "       3.14000000e+01, 2.37000000e+01, 4.22000000e+01, 7.56000000e+01,\n",
       "       4.18000000e+01, 4.36000000e+01, 4.90000000e+00, 3.56000000e+01,\n",
       "       2.65000000e+01, 4.90000000e+01, 3.39000000e+01, 3.07000000e+01,\n",
       "       5.30000000e+00, 1.07000000e+01, 6.41000000e+01, 5.04000000e+01,\n",
       "       4.55000000e+01, 6.90000000e+00, 6.88000000e+01, 7.76000000e+01,\n",
       "       8.98000000e+01, 6.98000000e+01, 3.25000000e+01, 1.32500000e+02,\n",
       "       1.57000000e+01, 2.25000000e+01, 2.40000000e+02, 4.12000000e+01,\n",
       "       6.30000000e+00, 5.82000000e+01, 3.79000000e+01, 2.95000000e+01,\n",
       "       5.20000000e+01, 2.15000000e+01, 2.16300000e+02, 1.25000000e+01,\n",
       "       2.30000000e+00, 5.16000000e+01, 2.07000000e+01, 6.34000000e+01,\n",
       "       7.42000000e+01, 2.32000000e+01, 4.34000000e+01, 5.60000000e+01,\n",
       "       4.06000000e+01, 4.78000000e+01, 4.84000000e+01, 6.92000000e+01,\n",
       "       5.56000000e+01, 6.18000000e+01, 7.30000000e+01, 8.34000000e+01,\n",
       "       3.78000000e+01, 1.26400000e+02, 4.72000000e+01, 5.92000000e+01,\n",
       "       1.20200000e+02, 5.62000000e+01, 8.90000000e+01, 7.62000000e+01,\n",
       "       8.28000000e+01, 6.96000000e+01, 7.20000000e+01, 4.08000000e+01,\n",
       "       1.07400000e+02, 7.16000000e+01, 2.99000000e+01, 1.42200000e+02,\n",
       "       7.98000000e+01, 8.48000000e+01, 1.05600000e+02, 5.76000000e+01,\n",
       "       4.50000000e+01, 6.22000000e+01, 1.11000000e+02, 1.22600000e+02,\n",
       "       5.48000000e+01, 6.10000000e+01, 6.16000000e+01, 1.56800000e+02,\n",
       "       3.99000000e+01, 2.10000000e+00, 1.90000000e+00, 1.10000000e+00,\n",
       "       4.70000000e+00, 5.70000000e+00, 9.22000000e+01, 1.15400000e+02,\n",
       "       1.01800000e+02, 4.96000000e+01, 8.56000000e+01, 8.14000000e+01,\n",
       "       8.16000000e+01, 6.76000000e+01, 5.84000000e+01, 4.16000000e+01,\n",
       "       5.64000000e+01, 1.11800000e+02, 4.74000000e+01, 1.13400000e+02,\n",
       "       7.40000000e+01, 7.06000000e+01, 7.84000000e+01, 6.86000000e+01,\n",
       "       7.74000000e+01, 7.32000000e+01, 9.94000000e+01, 6.52000000e+01,\n",
       "       8.82000000e+01, 5.74000000e+01, 9.52000000e+01, 5.70000000e+01,\n",
       "       1.19400000e+02, 1.05800000e+02, 5.44000000e+01, 4.14000000e+01,\n",
       "       9.44000000e+01, 6.44000000e+01, 9.38000000e+01, 6.84000000e+01,\n",
       "       5.28000000e+01, 8.50000000e+01, 8.32000000e+01, 1.06000000e+02,\n",
       "       6.02000000e+01, 9.10000000e+01, 5.36000000e+01, 1.04600000e+02,\n",
       "       8.04000000e+01, 1.06800000e+02, 8.86000000e+01, 7.28000000e+01,\n",
       "       6.28000000e+01, 4.66000000e+01, 9.56000000e+01, 5.30000000e+01,\n",
       "       5.42000000e+01, 1.10800000e+02, 5.78000000e+01, 1.23000000e+01,\n",
       "       2.29000000e+01, 1.33000000e+01, 1.34800000e+02, 8.44000000e+01,\n",
       "       4.62000000e+01, 6.42000000e+01, 8.68000000e+01, 1.57600000e+02,\n",
       "       1.66800000e+02, 1.56000000e+02, 1.14400000e+02, 8.18000000e+01,\n",
       "       2.25000000e+02, 8.70000000e+00, 9.14000000e+01, 1.81000000e+01,\n",
       "       4.92000000e+01, 6.08000000e+01, 9.72000000e+01, 8.26000000e+01,\n",
       "       7.86000000e+01, 5.38000000e+01, 7.26000000e+01, 1.39000000e+02,\n",
       "       5.88000000e+01, 6.50000000e+01, 7.90000000e+01, 7.58000000e+01,\n",
       "       5.52000000e+01, 1.92000000e+02, 8.08000000e+01, 1.18000000e+02,\n",
       "       9.64000000e+01, 1.64200000e+02, 6.94000000e+01, 7.52000000e+01,\n",
       "       5.54000000e+01, 8.70000000e+01, 9.30000000e+01, 9.16000000e+01,\n",
       "       8.42000000e+01, 8.76000000e+01, 7.80000000e+01, 7.34000000e+01,\n",
       "       1.02200000e+02, 6.04000000e+01, 8.94000000e+01, 1.07600000e+02,\n",
       "       8.38000000e+01, 7.82000000e+01, 8.80000000e+01, 9.50000000e+01,\n",
       "       7.04000000e+01, 6.64000000e+01, 8.22000000e+01, 1.55000000e+02,\n",
       "       6.00000000e+01, 6.56000000e+01, 6.30000000e+01, 8.40000000e+01,\n",
       "       8.74000000e+01, 8.30000000e+01, 7.36000000e+01, 7.72000000e+01,\n",
       "       7.94000000e+01, 1.21400000e+02, 6.06000000e+01, 9.36000000e+01,\n",
       "       1.68400000e+02, 1.45000000e+02, 9.40000000e+01, 8.92000000e+01,\n",
       "       6.82000000e+01, 1.82600000e+02, 1.10600000e+02, 9.80000000e+01,\n",
       "       2.78400000e+02, 1.40200000e+02, 6.20000000e+01, 1.21600000e+02,\n",
       "       7.18000000e+01, 1.15800000e+02, 7.92000000e+01, 2.68600000e+02,\n",
       "       1.83000000e+02, 1.09600000e+02, 1.28200000e+02, 2.06200000e+02,\n",
       "       1.36600000e+02, 1.50200000e+02, 1.61600000e+02, 1.09000000e+02,\n",
       "       1.15200000e+02, 1.75200000e+02, 2.47200000e+02, 8.54000000e+01,\n",
       "       1.05200000e+02, 7.48000000e+01, 1.15000000e+02, 1.26000000e+02,\n",
       "       8.46000000e+01, 1.58000000e+02, 7.78000000e+01, 6.46000000e+01,\n",
       "       1.18200000e+02, 1.43800000e+02, 1.18600000e+02, 1.12200000e+02,\n",
       "       1.78200000e+02, 1.83400000e+02, 1.08200000e+02, 1.55800000e+02,\n",
       "       1.11200000e+02, 1.28400000e+02, 8.02000000e+01, 1.29000000e+02,\n",
       "       1.05000000e+02, 1.77600000e+02, 8.62000000e+01, 2.36800000e+02,\n",
       "       9.34000000e+01, 7.96000000e+01, 2.06800000e+02, 8.12000000e+01,\n",
       "       9.26000000e+01, 1.65200000e+02, 1.70400000e+02, 9.32000000e+01,\n",
       "       9.82000000e+01, 1.48600000e+02, 7.02000000e+01, 1.44200000e+02,\n",
       "       1.64600000e+02, 1.74600000e+02, 8.78000000e+01, 1.24800000e+02,\n",
       "       1.20800000e+02, 7.88000000e+01, 7.66000000e+01, 1.19000000e+01,\n",
       "       9.50000000e+00, 1.05000000e+01, 7.10000000e+00, 6.50000000e+00,\n",
       "       6.10000000e+00, 3.43000000e+01, 3.90000000e+00, 6.70000000e+00,\n",
       "       1.27000000e+01, 1.59000000e+01, 1.45000000e+01, 2.93000000e+01,\n",
       "       2.39000000e+01, 8.90000000e+00, 9.10000000e+00, 1.31000000e+01,\n",
       "       1.09000000e+01, 1.85000000e+01, 6.54000000e+01, 7.50000000e+00,\n",
       "       4.73000000e+01, 5.90000000e+00, 1.97000000e+01, 1.21000000e+01,\n",
       "       1.15000000e+01, 7.90000000e+00, 2.53000000e+01, 1.75000000e+01,\n",
       "       7.30000000e+00, 9.90000000e+00, 1.55000000e+01, 8.10000000e+01,\n",
       "       9.58000000e+01, 9.04000000e+01, 1.41200000e+02, 1.62200000e+02,\n",
       "       1.36400000e+02, 1.31400000e+02, 1.32600000e+02, 3.67600000e+02,\n",
       "       1.84600000e+02, 1.82200000e+02, 1.45200000e+02, 9.68000000e+01,\n",
       "       7.46000000e+01, 1.29400000e+02, 1.22800000e+02, 1.09800000e+02,\n",
       "       1.45600000e+02, 2.10600000e+02, 1.17600000e+02, 6.95000000e+01,\n",
       "       1.67000000e+02, 6.90000000e+01])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values=df_map['Rainfall'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2d8c4ff0-ec86-4a0e-8b37-72fad2e00d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Sydney on 2015-02-10: {'MinTemp': 12.596466306993014, 'MaxTemp': 22.03611002545138, 'Rainfall': 1.3899400695087643}\n",
      "Predictions for Albury on 2015-02-15: {'MinTemp': 7.331554314754396, 'MaxTemp': 16.304999164447917, 'Rainfall': 4.359741996769704}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def predict_weather_by_location(df, year, month, day, location, window_size=7):\n",
    "    \"\"\"\n",
    "    Predict MaxTemp, MinTemp, and Rainfall for a specified date and location \n",
    "    using the last `window_size` days as input.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with 'Date', 'Location', and columns ['MinTemp', 'MaxTemp', 'Rainfall'].\n",
    "    - year (int): Year of the target prediction date.\n",
    "    - month (int): Month of the target prediction date.\n",
    "    - day (int): Day of the target prediction date.\n",
    "    - location (str): Location for which to predict the weather.\n",
    "    - window_size (int): Number of past days to use as input features.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Predicted values for MaxTemp, MinTemp, and Rainfall.\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame must contain the columns: {required_columns}\")\n",
    "    \n",
    "    # Ensure 'Date' column is datetime64 dtype\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        raise ValueError(\"Ensure 'Date' column is preprocessed as datetime64 dtype.\")\n",
    "    \n",
    "    # Filter data for the specified location\n",
    "    df_location = df[df['Location'] == location].copy()\n",
    "    if df_location.empty:\n",
    "        raise ValueError(f\"No data found for location '{location}'.\")\n",
    "\n",
    "    # Ensure the target date exists in the filtered data\n",
    "    target_date_str = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    target_date = pd.to_datetime(target_date_str)\n",
    "    if target_date not in df_location['Date'].values:\n",
    "        raise ValueError(f\"Target date {target_date_str} is not in the dataset for location '{location}'.\")\n",
    "\n",
    "    # Sort the data by date\n",
    "    df_location = df_location.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Get the target index and check window size availability\n",
    "    target_index = df_location.index[df_location['Date'] == target_date][0]\n",
    "    start_index = target_index - window_size\n",
    "    if start_index < 0:\n",
    "        raise ValueError(f\"Insufficient data to create sliding window for {target_date_str} at location '{location}'.\")\n",
    "\n",
    "    # Extract sliding window data\n",
    "    feature_columns = ['MinTemp', 'MaxTemp', 'Rainfall']\n",
    "    past_data = df_location[feature_columns].iloc[start_index:target_index].to_numpy()\n",
    "    input_features = past_data.flatten()\n",
    "\n",
    "    # Generate features for training\n",
    "    X = []\n",
    "    y = {col: [] for col in feature_columns}\n",
    "    for i in range(window_size, len(df_location)):\n",
    "        X.append(df_location[feature_columns].iloc[i - window_size:i].to_numpy().flatten())\n",
    "        for col in feature_columns:\n",
    "            y[col].append(df_location[col].iloc[i])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = {col: np.array(vals) for col, vals in y.items()}\n",
    "\n",
    "    # Train models for each target variable\n",
    "    models = {}\n",
    "    for col in feature_columns:\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y[col])\n",
    "        models[col] = model\n",
    "\n",
    "    # Predict the target values\n",
    "    predictions = {}\n",
    "    for col, model in models.items():\n",
    "        predictions[col] = model.predict([input_features])[0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "#df['Date'] = pd.to_datetime(df['Date'])  # Pre-convert 'Date' column to datetime\n",
    "\n",
    "# Predict for Sydney on 10th February 2015\n",
    "predicted_values = predict_weather_by_location(df_map, 2015, 5, 10, 'Sydney')\n",
    "print(f\"Predictions for Sydney on 2015-02-10: {predicted_values}\")\n",
    "\n",
    "# Predict for Melbourne on 15th February 2015\n",
    "predicted_values = predict_weather_by_location(df_map, 2015, 5, 10, 'Albury')\n",
    "print(f\"Predictions for Albury on 2015-02-15: {predicted_values}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "344863a6-c7d0-4d38-b9fc-987ce0f44260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping location 'Nhil': Target date 2013-01-10 is not in the dataset for location 'Nhil'.\n",
      "Skipping location 'Katherine': Target date 2013-01-10 is not in the dataset for location 'Katherine'.\n",
      "Skipping location 'Uluru': Target date 2013-01-10 is not in the dataset for location 'Uluru'.\n",
      "            Location        Date    MinTemp    MaxTemp  Rainfall\n",
      "0             Albury  2013-01-10  11.670822  29.274932  3.294615\n",
      "1      BadgerysCreek  2013-01-10  17.410805  27.831539  4.815527\n",
      "2              Cobar  2013-01-10  16.311503  31.704785  1.928623\n",
      "3       CoffsHarbour  2013-01-10  22.448657  31.830924 -4.998027\n",
      "4              Moree  2013-01-10  25.110734  38.794927  1.017792\n",
      "5          Newcastle  2013-01-10  17.800391  26.438580  6.765682\n",
      "6          NorahHead  2013-01-10  19.000192  25.389224  4.815879\n",
      "7      NorfolkIsland  2013-01-10  19.166731  24.486802  0.288469\n",
      "8            Penrith  2013-01-10  18.679032  28.790379  4.879299\n",
      "9           Richmond  2013-01-10  18.243853  27.949503  5.326152\n",
      "10            Sydney  2013-01-10  18.882821  25.307224  4.932224\n",
      "11     SydneyAirport  2013-01-10  17.717533  24.927412  2.695880\n",
      "12        WaggaWagga  2013-01-10  10.348831  29.911538  2.086566\n",
      "13       Williamtown  2013-01-10  19.014979  27.003068  6.074923\n",
      "14        Wollongong  2013-01-10  16.513225  22.739133  4.019463\n",
      "15          Canberra  2013-01-10  13.527491  27.827288  4.337687\n",
      "16       Tuggeranong  2013-01-10  13.496178  27.516369  4.276529\n",
      "17       MountGinini  2013-01-10   6.544297  19.975318  0.738067\n",
      "18          Ballarat  2013-01-10   6.983863  22.752947  1.293806\n",
      "19           Bendigo  2013-01-10   9.212967  27.251526  0.934470\n",
      "20              Sale  2013-01-10  11.031574  25.818815  1.404713\n",
      "21  MelbourneAirport  2013-01-10  10.746100  25.966752  0.779708\n",
      "22         Melbourne  2013-01-10  12.452770  25.312310  2.209962\n",
      "23           Mildura  2013-01-10  11.144879  32.652889  0.219167\n",
      "24          Portland  2013-01-10  11.192444  22.177997  0.817920\n",
      "25          Watsonia  2013-01-10  10.634747  25.821431  1.863703\n",
      "26          Dartmoor  2013-01-10   9.400058  25.095651  0.583980\n",
      "27          Brisbane  2013-01-10  22.295583  31.814481  1.158288\n",
      "28            Cairns  2013-01-10  23.531501  31.751898  7.875951\n",
      "29         GoldCoast  2013-01-10  23.023352  30.677738  1.431016\n",
      "30        Townsville  2013-01-10  24.147724  32.556750  1.575697\n",
      "31          Adelaide  2013-01-10  14.839752  29.426007  0.675066\n",
      "32      MountGambier  2013-01-10  10.013517  24.808064  0.721173\n",
      "33         Nuriootpa  2013-01-10  11.307530  28.090298  0.880565\n",
      "34           Woomera  2013-01-10  15.519019  33.332635  0.573182\n",
      "35            Albany  2013-01-10  17.428507  22.530735  1.925462\n",
      "36       Witchcliffe  2013-01-10  14.602855  25.620110 -2.314646\n",
      "37        PearceRAAF  2013-01-10  17.426645  28.980376  3.370721\n",
      "38      PerthAirport  2013-01-10  17.599777  27.522221  3.653280\n",
      "39             Perth  2013-01-10  17.977489  26.799466  2.904444\n",
      "40        SalmonGums  2013-01-10  21.136158  37.513701  1.154340\n",
      "41           Walpole  2013-01-10  15.664971  23.723717  2.009488\n",
      "42            Hobart  2013-01-10  10.624411  23.539616 -0.175578\n",
      "43        Launceston  2013-01-10   9.933401  24.209013  0.716614\n",
      "44      AliceSprings  2013-01-10  23.964495  42.021196  0.585651\n",
      "45            Darwin  2013-01-10  26.202426  33.537672  6.120278\n"
     ]
    }
   ],
   "source": [
    "def predict_weather_for_all_locations(df_map, year, month, day, window_size=7):\n",
    "    \"\"\"\n",
    "    Predict MaxTemp, MinTemp, and Rainfall for a specified date across all locations.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with 'Date', 'Location', and columns ['MinTemp', 'MaxTemp', 'Rainfall'].\n",
    "    - year (int): Year of the target prediction date.\n",
    "    - month (int): Month of the target prediction date.\n",
    "    - day (int): Day of the target prediction date.\n",
    "    - window_size (int): Number of past days to use as input features.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing predictions for all locations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    target_date_str = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    \n",
    "    # Iterate over all unique locations in the DataFrame\n",
    "    for location in df_map['Location'].unique():\n",
    "        try:\n",
    "            # Use the predict_weather_by_location function for each location\n",
    "            prediction = predict_weather_by_location(df_map, year, month, day, location, window_size)\n",
    "            # Append the results with location and target date info\n",
    "            results.append({\n",
    "                'Location': location,\n",
    "                'Date': target_date_str,\n",
    "                **prediction\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping location '{location}': {e}\")\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "\n",
    "predictions_df = predict_weather_for_all_locations(df_map, 2013, 1, 10)\n",
    "\n",
    "# Display results\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e5c9823d-e119-45ed-8681-a16c4282d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Geocoding\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in predictions_df['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the dataframe\n",
    "predictions_df.loc[:, 'Longitude'] = lons\n",
    "predictions_df.loc[:, 'Latitude'] = lats\n",
    "predictions_df = predictions_df.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract data\n",
    "lons = predictions_df['Longitude'].values\n",
    "lats = predictions_df['Latitude'].values\n",
    "max_temp = predictions_df['MaxTemp'].values\n",
    "min_temp = predictions_df['MinTemp'].values\n",
    "rainfall = predictions_df['Rainfall'].values\n",
    "\n",
    "# Get the date from the predictions_df\n",
    "forecast_date = predictions_df['Date'].iloc[0]  # Assuming 'Date' column exists and is consistent\n",
    "\n",
    "# Function to create interpolation map\n",
    "def create_interpolation_map(ax, lons, lats, values, title, cmap, cbar_label):\n",
    "    # Define a grid for interpolation\n",
    "    lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "    lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "    lon_grid, lat_grid = np.meshgrid(\n",
    "        np.linspace(lon_min, lon_max, 200),\n",
    "        np.linspace(lat_min, lat_max, 200)\n",
    "    )\n",
    "\n",
    "    # Perform Ordinary Kriging\n",
    "    krige = OrdinaryKriging(lons, lats, values, variogram_model='linear')\n",
    "    grid, _ = krige.execute('grid', lon_grid[0], lat_grid[:, 0])\n",
    "\n",
    "    # Add features to the map\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "    # Plot interpolated values\n",
    "    contour = ax.contourf(\n",
    "        lon_grid, lat_grid, grid,\n",
    "        levels=20, cmap=cmap, transform=ccrs.PlateCarree()\n",
    "    )\n",
    "\n",
    "    # Add data points\n",
    "    ax.scatter(lons, lats, edgecolors='k', s=50, label='Locations', c='white', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contour, ax=ax, orientation='vertical')\n",
    "    cbar.set_label(cbar_label)  # Colorbar only, no black label on map\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create side-by-side maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "create_interpolation_map(axes[0], lons, lats, max_temp, \"Max Temperature (°C)\", 'coolwarm', 'Temperature (°C)')\n",
    "create_interpolation_map(axes[1], lons, lats, min_temp, \"Min Temperature (°C)\", 'viridis', 'Temperature (°C)')\n",
    "create_interpolation_map(axes[2], lons, lats, rainfall, \"Rainfall (mm)\", 'Blues', 'Rainfall (mm)')\n",
    "\n",
    "# Add a descriptive label for the entire figure\n",
    "fig.text(0.5, 0.02, f\"Weather Forecast for Australia on {forecast_date}\", ha='center', fontsize=14)\n",
    "\n",
    "# Adjust layout: Add space between the maps\n",
    "plt.subplots_adjust(wspace=5)  # Increase wspace to add spacing between maps\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0.04, 1, 1])  # Leave space for the bottom label\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554462c",
   "metadata": {},
   "source": [
    "## Climate diagrams (10 year period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53837901",
   "metadata": {},
   "source": [
    "### X.1 Sydney weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e293eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope of the trend line: 0.0127\n",
      "Intercept of the trend line: 18.2546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Sydney weather station\n",
    "\n",
    "# combining year, month, and day into a single Date column\n",
    "#df_map['Date'] = pd.to_datetime(df_map[['Year', 'Month', 'Day']])\n",
    "\n",
    "# filtering for Sydney weather station and the required time period\n",
    "df_sydney = df_map[(df_map['Location'] == 'Sydney') & (df_map['Date'].dt.year >= 2007) & (df_map['Date'].dt.year <= 2017)]\n",
    "\n",
    "# Sort data by date\n",
    "df_sydney = df_sydney.sort_values(by='Date')\n",
    "\n",
    "# Calculate average temperature\n",
    "df_sydney['AvgTemp'] = (df_sydney['MaxTemp'] + df_sydney['MinTemp']) / 2 \n",
    "df_sydney['YearMonth'] = df_sydney['Date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate monthly data\n",
    "monthly_data = df_sydney.groupby('YearMonth').agg(\n",
    "    MonthlyMaxTemp=('MaxTemp', 'mean'),\n",
    "    MonthlyMinTemp=('MinTemp', 'mean'),\n",
    "    MonthlyAvgTemp=('AvgTemp', 'mean'),\n",
    "    TotalRainfall=('Rainfall', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Convert YearMonth to a datetime for plotting\n",
    "monthly_data['YearMonth'] = monthly_data['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Smooth data using a rolling window\n",
    "monthly_data['SmoothedMaxTemp'] = monthly_data['MonthlyMaxTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedMinTemp'] = monthly_data['MonthlyMinTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedAvgTemp'] = monthly_data['MonthlyAvgTemp'].rolling(window=3, center=True).mean()\n",
    "\n",
    "# Prepare data for linear regression\n",
    "monthly_data['NumericTime'] = np.arange(len(monthly_data))  # Numeric time for regression (e.g., 0, 1, 2,...)\n",
    "X = monthly_data[['NumericTime']]\n",
    "y = monthly_data['MonthlyAvgTemp']\n",
    "\n",
    "# Fit the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Predict the trend line\n",
    "monthly_data['TrendLine'] = reg.predict(X)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot Smoothed MaxTemp, MinTemp, and AvgTemp as line plots, also overlay regression trend line\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMaxTemp'], label='Smoothed max. temperature (°C)', color='red', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMinTemp'], label='Smoothed min. temperature (°C)', color='blue', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedAvgTemp'], label='Smoothed average temperature (°C)', color='green', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['TrendLine'], label='Average temperature trend line', color='purple', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Customize primary Y-axis (temperature)\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(alpha=0.5)\n",
    "\n",
    "# Create a secondary Y-axis for Rainfall\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    monthly_data['YearMonth'], \n",
    "    monthly_data['TotalRainfall'], \n",
    "    color='gray', alpha=0.6, label='Monthly rainfall (mm)', width=20\n",
    ")\n",
    "\n",
    "# Customize secondary Y-axis (rainfall)\n",
    "ax2.set_ylabel('Rainfall (mm)', fontsize=12)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Title and layout adjustments\n",
    "plt.title('Weather trends in Sydney (2007–2017)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print regression results\n",
    "print(f\"Slope of the trend line: {reg.coef_[0]:.4f}\")\n",
    "print(f\"Intercept of the trend line: {reg.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d69879e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driest year: 2017 with 865.80 mm of rainfall\n",
      "Wettest month: 2015-04 with 366.80 mm of rainfall\n",
      "Hottest year: 2017 with an average temperature of 20.76 °C\n",
      "Coldest year: 2008 with an average temperature of 17.71 °C\n",
      "Monthly averages:\n",
      "-------------------------\n",
      "        AvgTemp  Rainfall\n",
      "Date                     \n",
      "1     23.916846  3.127599\n",
      "2     23.533529  4.323922\n",
      "3     22.369355  4.483871\n",
      "4     19.440174  5.194815\n",
      "5     16.632097  2.434194\n",
      "6     14.436441  5.822240\n",
      "7     13.489068  2.853047\n",
      "8     14.535842  2.156272\n",
      "9     17.305730  1.819855\n",
      "10    19.123656  2.164734\n",
      "11    21.144248  2.905347\n",
      "12    22.192211  2.391935\n"
     ]
    }
   ],
   "source": [
    "# Driest year\n",
    "driest_year = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.year)['Rainfall'].sum()\n",
    "    .idxmin()\n",
    ")\n",
    "driest_year_rainfall = df_sydney.groupby(df_sydney['Date'].dt.year)['Rainfall'].sum().min()\n",
    "\n",
    "# Wettest month\n",
    "wettest_month = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.to_period('M'))['Rainfall'].sum()\n",
    "    .idxmax()\n",
    ")\n",
    "wettest_month_rainfall = df_sydney.groupby(df_sydney['Date'].dt.to_period('M'))['Rainfall'].sum().max()\n",
    "\n",
    "# Hottest year\n",
    "hottest_year = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmax()\n",
    ")\n",
    "hottest_year_temp = df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean().max()\n",
    "\n",
    "# Coldest year\n",
    "coldest_year = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmin()\n",
    ")\n",
    "coldest_year_temp = df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean().min()\n",
    "\n",
    "# Monthly averages across all years\n",
    "monthly_avg = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.month)[['AvgTemp', 'Rainfall']].mean()\n",
    ")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Driest year: {driest_year} with {driest_year_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Wettest month: {wettest_month} with {wettest_month_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Hottest year: {hottest_year} with an average temperature of {hottest_year_temp:.2f} °C\")\n",
    "print(f\"Coldest year: {coldest_year} with an average temperature of {coldest_year_temp:.2f} °C\")\n",
    "print(\"Monthly averages:\")\n",
    "print('-------------------------')\n",
    "print(monthly_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44be48",
   "metadata": {},
   "source": [
    "## X.2 Darwin weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0b008b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope of the trend line: 0.0042\n",
      "Intercept of the trend line: 27.6526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Darwuin weather station\n",
    "\n",
    "# combining year, month, and day into a single Date column\n",
    "#df_map['Date'] = pd.to_datetime(df_map[['Year', 'Month', 'Day']])\n",
    "\n",
    "# filtering for Darwin weather station and the required time period\n",
    "df_darwin = df_map[(df_map['Location'] == 'Darwin') & (df_map['Date'].dt.year >= 2007) & (df_map['Date'].dt.year <= 2017)]\n",
    "\n",
    "# Sort data by date\n",
    "df_darwin = df_darwin.sort_values(by='Date')\n",
    "\n",
    "# Calculate average temperature\n",
    "df_darwin['AvgTemp'] = (df_darwin['MaxTemp'] + df_darwin['MinTemp']) / 2 \n",
    "df_darwin['YearMonth'] = df_darwin['Date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate monthly data\n",
    "monthly_data = df_darwin.groupby('YearMonth').agg(\n",
    "    MonthlyMaxTemp=('MaxTemp', 'mean'),\n",
    "    MonthlyMinTemp=('MinTemp', 'mean'),\n",
    "    MonthlyAvgTemp=('AvgTemp', 'mean'),\n",
    "    TotalRainfall=('Rainfall', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Convert YearMonth to a datetime for plotting\n",
    "monthly_data['YearMonth'] = monthly_data['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Smooth data using a rolling window\n",
    "monthly_data['SmoothedMaxTemp'] = monthly_data['MonthlyMaxTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedMinTemp'] = monthly_data['MonthlyMinTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedAvgTemp'] = monthly_data['MonthlyAvgTemp'].rolling(window=3, center=True).mean()\n",
    "\n",
    "# Prepare data for linear regression\n",
    "monthly_data['NumericTime'] = np.arange(len(monthly_data))  # Numeric time for regression (e.g., 0, 1, 2,...)\n",
    "X = monthly_data[['NumericTime']]\n",
    "y = monthly_data['MonthlyAvgTemp']\n",
    "\n",
    "# Fit the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Predict the trend line\n",
    "monthly_data['TrendLine'] = reg.predict(X)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot Smoothed MaxTemp, MinTemp, and AvgTemp as line plots, also overlay regression trend line\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMaxTemp'], label='Smoothed max. temperature (°C)', color='red', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMinTemp'], label='Smoothed min. temperature (°C)', color='blue', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedAvgTemp'], label='Smoothed average temperature (°C)', color='green', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['TrendLine'], label='Average temperature trend line', color='purple', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Customize primary Y-axis (temperature)\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(alpha=0.5)\n",
    "\n",
    "# Create a secondary Y-axis for Rainfall\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    monthly_data['YearMonth'], \n",
    "    monthly_data['TotalRainfall'], \n",
    "    color='gray', alpha=0.6, label='Monthly rainfall (mm)', width=20\n",
    ")\n",
    "\n",
    "# Customize secondary Y-axis (rainfall)\n",
    "ax2.set_ylabel('Rainfall (mm)', fontsize=12)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Title and layout adjustments\n",
    "plt.title('Weather trends in Darwin (2007–2017)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print regression results\n",
    "print(f\"Slope of the trend line: {reg.coef_[0]:.4f}\")\n",
    "print(f\"Intercept of the trend line: {reg.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "aafab1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driest year: 2008 with nan mm of rainfall\n",
      "Wettest month: 2011-02 with 1110.20 mm of rainfall\n",
      "Hottest year: 2016 with an average temperature of nan °C\n",
      "Coldest year: 2011 with an average temperature of 26.91 °C\n",
      "Monthly averages:\n",
      "-------------------------\n",
      "        AvgTemp  Rainfall\n",
      "Date                     \n",
      "1     23.916846  3.127599\n",
      "2     23.533529  4.323922\n",
      "3     22.369355  4.483871\n",
      "4     19.440174  5.194815\n",
      "5     16.632097  2.434194\n",
      "6     14.436441  5.822240\n",
      "7     13.489068  2.853047\n",
      "8     14.535842  2.156272\n",
      "9     17.305730  1.819855\n",
      "10    19.123656  2.164734\n",
      "11    21.144248  2.905347\n",
      "12    22.192211  2.391935\n"
     ]
    }
   ],
   "source": [
    "# Driest year\n",
    "driest_year = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.year)['Rainfall'].sum()\n",
    "    .idxmin()\n",
    ")\n",
    "driest_year_rainfall = df_darwin.groupby(df_sydney['Date'].dt.year)['Rainfall'].sum().min()\n",
    "\n",
    "# Wettest month\n",
    "wettest_month = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.to_period('M'))['Rainfall'].sum()\n",
    "    .idxmax()\n",
    ")\n",
    "wettest_month_rainfall = df_darwin.groupby(df_darwin['Date'].dt.to_period('M'))['Rainfall'].sum().max()\n",
    "\n",
    "# Hottest year\n",
    "hottest_year = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmax()\n",
    ")\n",
    "hottest_year_temp = df_darwin.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean().max()\n",
    "\n",
    "# Coldest year\n",
    "coldest_year = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmin()\n",
    ")\n",
    "coldest_year_temp = df_darwin.groupby(df_darwin['Date'].dt.year)['AvgTemp'].mean().min()\n",
    "\n",
    "# Monthly averages across all years\n",
    "monthly_avg = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.month)[['AvgTemp', 'Rainfall']].mean()\n",
    ")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Driest year: {driest_year} with {driest_year_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Wettest month: {wettest_month} with {wettest_month_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Hottest year: {hottest_year} with an average temperature of {hottest_year_temp:.2f} °C\")\n",
    "print(f\"Coldest year: {coldest_year} with an average temperature of {coldest_year_temp:.2f} °C\")\n",
    "print(\"Monthly averages:\")\n",
    "print('-------------------------')\n",
    "print(monthly_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "54b651ee-519e-4654-83f1-89549b485f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Predictions vs Actual Weather by Location on 2013-01-10 00:00:00\n",
      "            Location MinTemp_Diff MaxTemp_Diff Rainfall_Diff\n",
      "                             mean         mean          mean\n",
      "0           Adelaide     1.239752    -4.473993      0.675066\n",
      "1             Albany    -0.871493     1.430735      1.925462\n",
      "2             Albury     0.470822    -2.925068      3.294615\n",
      "3       AliceSprings     2.264495    -0.078804      0.585651\n",
      "4      BadgerysCreek    -0.989195     1.131539      4.815527\n",
      "5           Ballarat    -0.116137    -6.347053      1.293806\n",
      "6            Bendigo     0.212967    -3.248474      0.934470\n",
      "7           Brisbane    -1.204417     0.114481      1.158288\n",
      "8             Cairns     1.031501    -0.348102      7.875951\n",
      "9           Canberra     3.527491    -1.872712      4.337687\n",
      "10             Cobar     0.711503    -3.195215      1.928623\n",
      "11      CoffsHarbour     1.548657     4.030924     -7.798027\n",
      "12          Dartmoor    -0.699942    -3.104349      0.583980\n",
      "13            Darwin    -1.697574     0.237672      6.120278\n",
      "14         GoldCoast    -0.976648     0.477738      1.431016\n",
      "15            Hobart    -0.375589     1.439616     -0.175578\n",
      "16        Launceston     1.533401     3.909013      0.716614\n",
      "17         Melbourne    -1.747230    -1.887690      2.009962\n",
      "18  MelbourneAirport     2.046100    -1.833248      0.779708\n",
      "19           Mildura    -0.655121     0.052889      0.219167\n",
      "20             Moree     8.410734    -0.905073      1.017792\n",
      "21      MountGambier     2.013517    -2.991936      0.721173\n",
      "22       MountGinini     2.544297    -0.224682      0.738067\n",
      "23         Newcastle     5.606356     0.038580      6.765682\n",
      "24         NorahHead    -1.199808    -0.110776      4.815879\n",
      "25     NorfolkIsland    -0.833269    -1.513198      0.288469\n",
      "26         Nuriootpa     0.607530    -6.509702      0.880565\n",
      "27        PearceRAAF     3.026645     0.080376      1.009803\n",
      "28           Penrith    -0.720968     1.890379      4.879299\n",
      "29             Perth     2.377489     0.599466      2.904444\n",
      "30      PerthAirport     4.599777     0.122221      3.653280\n",
      "31          Portland     0.992444     1.977997      0.817920\n",
      "32          Richmond    -1.156147     0.649503      5.326152\n",
      "33              Sale     3.931574     2.618815      1.404713\n",
      "34        SalmonGums     4.536158     8.413701      1.154340\n",
      "35            Sydney    -1.317179    -0.092776      4.932224\n",
      "36     SydneyAirport    -1.982467    -1.172588      2.695880\n",
      "37        Townsville    -0.252276    -0.443250      1.575697\n",
      "38       Tuggeranong     0.796178    -1.883631      3.876529\n",
      "39        WaggaWagga    -0.251169    -2.888462      2.086566\n",
      "40           Walpole    -1.035029     0.723717      2.009488\n",
      "41          Watsonia    -2.865253    -4.678569      1.863703\n",
      "42       Williamtown    -1.585021     1.103068      6.074923\n",
      "43       Witchcliffe    -2.497145     1.020110     -2.314646\n",
      "44        Wollongong    -1.286775    -2.860867      4.019463\n",
      "45           Woomera    -0.780981    -3.667365      0.573182\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "\n",
    "#Comparison of the sliding window weather forecast with the actual weather observations on the date\n",
    "\n",
    "df_map['Date'] = pd.to_datetime(df_map['Date'])\n",
    "predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])\n",
    "\n",
    "# Merge the dataframes on 'Date' and 'Location'\n",
    "comparison_df = pd.merge(\n",
    "    predictions_df, \n",
    "    df_map, \n",
    "    on=['Date', 'Location'], \n",
    "    suffixes=('_predicted', '_actual')\n",
    ")\n",
    "\n",
    "# Calculate differences\n",
    "comparison_df['MinTemp_Diff'] = comparison_df['MinTemp_predicted'] - comparison_df['MinTemp_actual']\n",
    "comparison_df['MaxTemp_Diff'] = comparison_df['MaxTemp_predicted'] - comparison_df['MaxTemp_actual']\n",
    "comparison_df['Rainfall_Diff'] = comparison_df['Rainfall_predicted'] - comparison_df['Rainfall_actual']\n",
    "\n",
    "# Group by location and summarize the differences\n",
    "summary = comparison_df.groupby('Location').agg({\n",
    "    'MinTemp_Diff': ['mean'],\n",
    "    'MaxTemp_Diff': ['mean'],\n",
    "    'Rainfall_Diff': ['mean']\n",
    "})\n",
    "\n",
    "# Reset index for easy viewing\n",
    "summary = summary.reset_index()\n",
    "\n",
    "prediction_date= predictions_df['Date'].iloc[1]\n",
    "# Display the summary\n",
    "print(f\"Comparison of Predictions vs Actual Weather by Location on {prediction_date}\" )\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
