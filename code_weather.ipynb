{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9c86634d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
      "0           W           44.0          W  ...        71.0         22.0   \n",
      "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
      "2         WSW           46.0          W  ...        38.0         30.0   \n",
      "3          NE           24.0         SE  ...        45.0         16.0   \n",
      "4           W           41.0        ENE  ...        82.0         33.0   \n",
      "\n",
      "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
      "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
      "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
      "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
      "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
      "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
      "\n",
      "   RainTomorrow  \n",
      "0            No  \n",
      "1            No  \n",
      "2            No  \n",
      "3            No  \n",
      "4            No  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "There are 7 categorical variables\n",
      "\n",
      "Categorical variables are : ['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import geopandas as gpd    \n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # data visualization\n",
    "sns.reset_defaults()\n",
    "import geoplot as gplt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.interpolate import griddata\n",
    "#read data\n",
    "\n",
    "df = pd.read_csv(\"weatherAUS.csv\")\n",
    "print(df.head())\n",
    "#df.info()\n",
    "\n",
    "\n",
    "## Find categorical variables\n",
    "\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('Categorical variables are :', categorical)\n",
    "\n",
    "## find missing values in categorical variables\n",
    "\n",
    "#print(df[categorical].isnull().sum())\n",
    "\n",
    "##frequency of categorical variables\n",
    "\n",
    "#for var in categorical: \n",
    "        \n",
    "#print(df[var].value_counts())\n",
    "\n",
    "##check for cardinality in categorical variables\n",
    "\n",
    "#for var in categorical:\n",
    "    \n",
    "#    print(var, ' contains ', len(df[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "17dafde9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## date variable contains 3436 labels so needs to be split into year/month/day\n",
    "\n",
    "#print(df[\"Date\"].dtypes)\n",
    "\n",
    "df['Date']= pd.to_datetime(df['Date'])\n",
    "\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "df.drop('Date', axis=1, inplace = True)\n",
    "\n",
    "#start looking into other categorical variables\n",
    "\n",
    "#print('Location contains', len(df.Location.unique()), 'labels')\n",
    "\n",
    "#print(df.Location.unique())\n",
    "\n",
    "#one-hot encoding for categorical variables\n",
    "\n",
    "# add most popular values for missing categorical values\n",
    "\n",
    "for df2 in [df]:\n",
    "    df2['WindGustDir'] = df2['WindGustDir'].fillna(df2['WindGustDir'].mode()[0])\n",
    "    df2['WindDir9am'] = df2['WindDir9am'].fillna(df2['WindDir9am'].mode()[0])\n",
    "    df2['WindDir3pm'] = df2['WindDir3pm'].fillna(df2['WindDir3pm'].mode()[0])\n",
    "    df2['RainToday'] = df2['RainToday'].fillna(df2['RainToday'].mode()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ca6d5e79-e107-436b-ab4c-5c3a96deed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## kaartide jaoks location vajalik\n",
    "df_map = pd.get_dummies(df, columns=[ 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'], drop_first=True, dummy_na=True)\n",
    "\n",
    "## muude toimetuste jaoks\n",
    "df = pd.get_dummies(df, columns=['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'], drop_first=True, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b899b-7a93-42c0-84bf-51ea3f40c0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bcfa8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting column 'RainTomorrow' to numeric variable\n",
    "\n",
    "df['RainTomorrow'] = df['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "## Handling missing values in column 'RainTomorrow'\n",
    "df = df.dropna(subset=['RainTomorrow'])  # Drop rows with NaN in target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a541ea72-8513-48a3-bde0-ca36846bbada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTemp             637\n",
      "MaxTemp             322\n",
      "Rainfall           1406\n",
      "Evaporation       60843\n",
      "Sunshine          67816\n",
      "                  ...  \n",
      "WindDir3pm_WNW        0\n",
      "WindDir3pm_WSW        0\n",
      "WindDir3pm_nan        0\n",
      "RainToday_Yes         0\n",
      "RainToday_nan         0\n",
      "Length: 119, dtype: int64\n",
      "        MinTemp   MaxTemp  Rainfall  Evaporation  Sunshine  WindGustSpeed  \\\n",
      "count  141556.0  141871.0  140787.0      81350.0   74377.0       132923.0   \n",
      "mean       12.0      23.0       2.0          5.0       8.0           40.0   \n",
      "std         6.0       7.0       8.0          4.0       4.0           14.0   \n",
      "min        -8.0      -5.0       0.0          0.0       0.0            6.0   \n",
      "25%         8.0      18.0       0.0          3.0       5.0           31.0   \n",
      "50%        12.0      23.0       0.0          5.0       8.0           39.0   \n",
      "75%        17.0      28.0       1.0          7.0      11.0           48.0   \n",
      "max        34.0      48.0     371.0        145.0      14.0          135.0   \n",
      "\n",
      "       WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  Pressure9am  \\\n",
      "count      140845.0      139563.0     140419.0     138583.0     128179.0   \n",
      "mean           14.0          19.0         69.0         51.0       1018.0   \n",
      "std             9.0           9.0         19.0         21.0          7.0   \n",
      "min             0.0           0.0          0.0          0.0        980.0   \n",
      "25%             7.0          13.0         57.0         37.0       1013.0   \n",
      "50%            13.0          19.0         70.0         52.0       1018.0   \n",
      "75%            19.0          24.0         83.0         66.0       1022.0   \n",
      "max           130.0          87.0        100.0        100.0       1041.0   \n",
      "\n",
      "       Pressure3pm  Cloud9am  Cloud3pm   Temp9am   Temp3pm  RainTomorrow  \\\n",
      "count     128212.0   88536.0   85099.0  141289.0  139467.0      142193.0   \n",
      "mean        1015.0       4.0       5.0      17.0      22.0           0.0   \n",
      "std            7.0       3.0       3.0       6.0       7.0           0.0   \n",
      "min          977.0       0.0       0.0      -7.0      -5.0           0.0   \n",
      "25%         1010.0       1.0       2.0      12.0      17.0           0.0   \n",
      "50%         1015.0       5.0       5.0      17.0      21.0           0.0   \n",
      "75%         1020.0       7.0       7.0      22.0      26.0           0.0   \n",
      "max         1040.0       9.0       9.0      40.0      47.0           1.0   \n",
      "\n",
      "           Year     Month       Day  \n",
      "count  142193.0  142193.0  142193.0  \n",
      "mean     2013.0       6.0      16.0  \n",
      "std         3.0       3.0       9.0  \n",
      "min      2007.0       1.0       1.0  \n",
      "25%      2011.0       3.0       8.0  \n",
      "50%      2013.0       6.0      16.0  \n",
      "75%      2015.0       9.0      23.0  \n",
      "max      2017.0      12.0      31.0   2\n"
     ]
    }
   ],
   "source": [
    "#explore numerical variables\n",
    "\n",
    "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "\n",
    "#print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "#print('The numerical variables are :', numerical)\n",
    "\n",
    "#19 numerical variables, all continuous type\n",
    "#check for missing values\n",
    "\n",
    "print(df[numerical].isnull().sum())\n",
    "\n",
    "print(round(df[numerical].describe()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85efc07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b081edfd-1a98-4a63-8b62-6e164469ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plots\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = df.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = df.boxplot(column='Evaporation')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Evaporation')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "fig = df.boxplot(column='WindSpeed9am')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed9am')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "fig = df.boxplot(column='WindSpeed3pm')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed3pm')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5b248c66-c568-4044-a145-7b24ef654327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting into prediction stuff\n",
    "\n",
    "X = df.drop(['RainTomorrow'], axis=1)\n",
    "\n",
    "y = df['RainTomorrow']\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Replacing missing values?\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "num_imputer = SimpleImputer(strategy='mean')  # Replace NaNs with mean for numerical features\n",
    "X[numerical_cols] = num_imputer.fit_transform(X[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a4e343a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((113754, 118), (28439, 118))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 15)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "908613bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8473575020218713\n",
      "[[20827  1185]\n",
      " [ 3156  3271]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91     22012\n",
      "         1.0       0.73      0.51      0.60      6427\n",
      "\n",
      "    accuracy                           0.85     28439\n",
      "   macro avg       0.80      0.73      0.75     28439\n",
      "weighted avg       0.84      0.85      0.84     28439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Scaling data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## Initializing logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model_logreg = LogisticRegression()\n",
    "model_logreg.fit(X_train, y_train)\n",
    "\n",
    "## Predictions and evaluation\n",
    "y_pred = model_logreg.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "77c4351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaardi genemine (see on lic temperatuuride kaart kindlal ajahetkel)\n",
    "## peab kasutama df_map!!!\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# 1. Filter data for the specific date\n",
    "df_filtered = df_map[(df_map['Year'] == 2012) & (df_map['Month'] == 4) & (df_map['Day'] == 30)].copy()\n",
    "\n",
    "# 2. Geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# Create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in df_filtered['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# 3. Add latitude and longitude to the filtered dataframe\n",
    "df_filtered.loc[:, 'Longitude'] = lons\n",
    "df_filtered.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# 4. Drop rows with missing coordinates or temperature data\n",
    "df_filtered = df_filtered.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# 5. Extract columns for plotting\n",
    "lons = df_filtered['Longitude']\n",
    "lats = df_filtered['Latitude']\n",
    "temps = df_filtered['MaxTemp']\n",
    "\n",
    "# 6. Create the map plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# Plot temperature points\n",
    "sc = ax.scatter(lons, lats, c=temps, cmap='coolwarm', edgecolors='k', s=100)\n",
    "\n",
    "# Add temperature labels\n",
    "for idx, row in df_filtered.iterrows():\n",
    "    ax.text(row['Longitude'], row['Latitude'] + 0.2, f\"{row['MaxTemp']}°C\", fontsize=10, ha='center', color='black')  # Temp above the point\n",
    "    ax.text(row['Longitude'], row['Latitude'] - 0.2, row['Location'], fontsize=9, ha='center', color='blue')         # Location below the point\n",
    "\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax, orientation='vertical', label='Max Temperature (°C)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Max Temperature across Australia on 30 April 2012\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d11e2aad-175f-4962-8893-413937b26327",
   "metadata": {},
   "outputs": [],
   "source": [
    "##next map is better for the poster\n",
    "\n",
    "## average max temp for all weather stations for january \n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "krige = OrdinaryKriging(lons, lats, temps, variogram_model='linear')\n",
    "\n",
    "# 1. Filter the dataset for January\n",
    "df_january = df_map[df_map['Month'] == 1].copy()\n",
    "\n",
    "# 2. Drop rows with missing temperature data\n",
    "df_january = df_january.dropna(subset=['MaxTemp'])\n",
    "\n",
    "# 3. Group by weather station and calculate the average temperature\n",
    "# Assuming 'Location' represents the weather station\n",
    "avg_temps_january = df_january.groupby('Location')['MaxTemp'].mean().reset_index()\n",
    "\n",
    "# 4. Rename columns for clarity\n",
    "avg_temps_january.columns = ['Location', 'MaxTemp']\n",
    "\n",
    "# 5. Display the result\n",
    "##print(avg_temps_january)\n",
    "\n",
    "\n",
    "# 2. Geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# Create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in avg_temps_january['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the filtered dataframe using .loc\n",
    "avg_temps_january.loc[:, 'Longitude'] = lons\n",
    "avg_temps_january.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# Drop rows with NaN coordinates or missing MaxTemp values\n",
    "avg_temps_january = avg_temps_january.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract data for interpolation\n",
    "lons = avg_temps_january['Longitude'].values\n",
    "lats = avg_temps_january['Latitude'].values\n",
    "temps = avg_temps_january['MaxTemp'].values\n",
    "\n",
    "# 3. Define a grid for interpolation\n",
    "lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "lon_grid, lat_grid = np.meshgrid(\n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "# 4. Interpolate temperature values onto the grid\n",
    "temp_grid, _ = krige.execute(\n",
    "    'grid', \n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 5. Plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "# Plot interpolated temperature as a contour map\n",
    "contour = ax.contourf(\n",
    "    lon_grid, lat_grid, temp_grid, \n",
    "    levels=20, cmap='coolwarm', transform=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "sc = ax.scatter(\n",
    "    lons, lats, color='black', edgecolors='k', s=100, label='Data Points'\n",
    ")\n",
    "\n",
    "# Add location labels\n",
    "for idx, row in avg_temps_january.iterrows():\n",
    "    ax.text(\n",
    "        row['Longitude'], row['Latitude'] + 0.5,  # Slightly offset above each point\n",
    "        row['Location'], fontsize=9, ha='center', color='green'\n",
    "    )\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Interpolated average Max Temperature across Australia in january\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1ac96-30c6-457d-bd9f-a97695a9e152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3440d797-2ae9-4a61-9c48-498240564198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#läheb postrile\n",
    "#interpoleeriv kaart, pead täpsustama kuupäva df filtered all, kriging näeb hea välja\n",
    "# kasuta df_map\n",
    "#pip install pykrige\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "krige = OrdinaryKriging(lons, lats, temps, variogram_model='linear')\n",
    "\n",
    "\n",
    "# 1. Filter data for a specific date\n",
    "df_filtered = df_map[(df_map['Year'] == 2015) & (df_map['Month'] == 7) & (df_map['Day'] == 8)].copy()\n",
    "\n",
    "# 2. Geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# Create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in df_filtered['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the filtered dataframe using .loc\n",
    "df_filtered.loc[:, 'Longitude'] = lons\n",
    "df_filtered.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# Drop rows with NaN coordinates or missing MaxTemp values\n",
    "df_filtered = df_filtered.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract data for interpolation\n",
    "lons = df_filtered['Longitude'].values\n",
    "lats = df_filtered['Latitude'].values\n",
    "temps = df_filtered['MaxTemp'].values\n",
    "\n",
    "# 3. Define a grid for interpolation\n",
    "lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "lon_grid, lat_grid = np.meshgrid(\n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "# 4. Interpolate temperature values onto the grid\n",
    "temp_grid, _ = krige.execute(\n",
    "    'grid', \n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 5. Plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# Plot interpolated temperature as a contour map\n",
    "contour = ax.contourf(\n",
    "    lon_grid, lat_grid, temp_grid, \n",
    "    levels=20, cmap='coolwarm', transform=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "# Add the original data points\n",
    "sc = ax.scatter(lons, lats, edgecolors='k', s=100, label='Location')\n",
    "\n",
    "# Add a colorbar for the temperature scale\n",
    "cbar = plt.colorbar(contour, ax=ax, orientation='vertical', label='Max Temperature (°C)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Interpolated Max Temperature across Australia \")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "185c6d90-e45e-44df-bfab-55b03e1bb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge year month day back together\n",
    "df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "# Drop Year, Month, Day columns if not needed\n",
    "df = df.drop(['Year', 'Month', 'Day'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "deadbaa3-c3ad-4284-ac58-e777eb8543a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df.select_dtypes(include=['float', 'int']).columns\n",
    "num_imputer = SimpleImputer(strategy='mean')  # Replace NaNs with mean for numerical features\n",
    "df[numerical_cols] = num_imputer.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "83085cc5-5738-4d9b-8e11-5018dacc9dbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Day'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Day'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[216], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Day'"
     ]
    }
   ],
   "source": [
    "df['Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "24e48b04-2f2e-455f-8abc-12eae91de467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2008-12-01\n",
       "1        2008-12-02\n",
       "2        2008-12-03\n",
       "3        2008-12-04\n",
       "4        2008-12-05\n",
       "            ...    \n",
       "145455   2017-06-21\n",
       "145456   2017-06-22\n",
       "145457   2017-06-23\n",
       "145458   2017-06-24\n",
       "145459   2017-06-25\n",
       "Name: Date, Length: 145460, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "04e7ad95-36ce-48c2-aeed-ed234a6ca1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n",
       "       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n",
       "       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n",
       "       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n",
       "       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n",
       "       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n",
       "       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n",
       "       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n",
       "       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n",
       "       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values=df_map['Location'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4fdca060-f1c5-43ce-b6be-2a87ff418f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Year', 'Month', 'Day'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Year', 'Month', 'Day'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "print(df[['Year', 'Month', 'Day']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2d8c4ff0-ec86-4a0e-8b37-72fad2e00d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Sydney on 2015-02-10: {'MinTemp': 12.596466306993014, 'MaxTemp': 22.03611002545138, 'Rainfall': 1.3899400695087643}\n",
      "Predictions for Albury on 2015-02-15: {'MinTemp': 7.331554314754396, 'MaxTemp': 16.304999164447917, 'Rainfall': 4.359741996769704}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def predict_weather_by_location(df, year, month, day, location, window_size=7):\n",
    "    \"\"\"\n",
    "    Predict MaxTemp, MinTemp, and Rainfall for a specified date and location \n",
    "    using the last `window_size` days as input.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with 'Date', 'Location', and columns ['MinTemp', 'MaxTemp', 'Rainfall'].\n",
    "    - year (int): Year of the target prediction date.\n",
    "    - month (int): Month of the target prediction date.\n",
    "    - day (int): Day of the target prediction date.\n",
    "    - location (str): Location for which to predict the weather.\n",
    "    - window_size (int): Number of past days to use as input features.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Predicted values for MaxTemp, MinTemp, and Rainfall.\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame must contain the columns: {required_columns}\")\n",
    "    \n",
    "    # Ensure 'Date' column is datetime64 dtype\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        raise ValueError(\"Ensure 'Date' column is preprocessed as datetime64 dtype.\")\n",
    "    \n",
    "    # Filter data for the specified location\n",
    "    df_location = df[df['Location'] == location].copy()\n",
    "    if df_location.empty:\n",
    "        raise ValueError(f\"No data found for location '{location}'.\")\n",
    "\n",
    "    # Ensure the target date exists in the filtered data\n",
    "    target_date_str = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    target_date = pd.to_datetime(target_date_str)\n",
    "    if target_date not in df_location['Date'].values:\n",
    "        raise ValueError(f\"Target date {target_date_str} is not in the dataset for location '{location}'.\")\n",
    "\n",
    "    # Sort the data by date\n",
    "    df_location = df_location.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Get the target index and check window size availability\n",
    "    target_index = df_location.index[df_location['Date'] == target_date][0]\n",
    "    start_index = target_index - window_size\n",
    "    if start_index < 0:\n",
    "        raise ValueError(f\"Insufficient data to create sliding window for {target_date_str} at location '{location}'.\")\n",
    "\n",
    "    # Extract sliding window data\n",
    "    feature_columns = ['MinTemp', 'MaxTemp', 'Rainfall']\n",
    "    past_data = df_location[feature_columns].iloc[start_index:target_index].to_numpy()\n",
    "    input_features = past_data.flatten()\n",
    "\n",
    "    # Generate features for training\n",
    "    X = []\n",
    "    y = {col: [] for col in feature_columns}\n",
    "    for i in range(window_size, len(df_location)):\n",
    "        X.append(df_location[feature_columns].iloc[i - window_size:i].to_numpy().flatten())\n",
    "        for col in feature_columns:\n",
    "            y[col].append(df_location[col].iloc[i])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = {col: np.array(vals) for col, vals in y.items()}\n",
    "\n",
    "    # Train models for each target variable\n",
    "    models = {}\n",
    "    for col in feature_columns:\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y[col])\n",
    "        models[col] = model\n",
    "\n",
    "    # Predict the target values\n",
    "    predictions = {}\n",
    "    for col, model in models.items():\n",
    "        predictions[col] = model.predict([input_features])[0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "#df['Date'] = pd.to_datetime(df['Date'])  # Pre-convert 'Date' column to datetime\n",
    "\n",
    "# Predict for Sydney on 10th February 2015\n",
    "predicted_values = predict_weather_by_location(df, 2015, 5, 10, 'Sydney')\n",
    "print(f\"Predictions for Sydney on 2015-02-10: {predicted_values}\")\n",
    "\n",
    "# Predict for Melbourne on 15th February 2015\n",
    "predicted_values = predict_weather_by_location(df, 2015, 5, 10, 'Albury')\n",
    "print(f\"Predictions for Albury on 2015-02-15: {predicted_values}\")\n",
    "\n",
    "\n",
    "#loe kokku ühte df-i ja tee kaart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "344863a6-c7d0-4d38-b9fc-987ce0f44260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping location 'Nhil': Target date 2013-01-10 is not in the dataset for location 'Nhil'.\n",
      "Skipping location 'Katherine': Target date 2013-01-10 is not in the dataset for location 'Katherine'.\n",
      "Skipping location 'Uluru': Target date 2013-01-10 is not in the dataset for location 'Uluru'.\n",
      "            Location        Date    MinTemp    MaxTemp  Rainfall\n",
      "0             Albury  2013-01-10  11.670822  29.274932  3.294615\n",
      "1      BadgerysCreek  2013-01-10  17.410805  27.831539  4.815527\n",
      "2              Cobar  2013-01-10  16.311503  31.704785  1.928623\n",
      "3       CoffsHarbour  2013-01-10  22.448657  31.830924 -4.998027\n",
      "4              Moree  2013-01-10  25.110734  38.794927  1.017792\n",
      "5          Newcastle  2013-01-10  17.800391  26.438580  6.765682\n",
      "6          NorahHead  2013-01-10  19.000192  25.389224  4.815879\n",
      "7      NorfolkIsland  2013-01-10  19.166731  24.486802  0.288469\n",
      "8            Penrith  2013-01-10  18.679032  28.790379  4.879299\n",
      "9           Richmond  2013-01-10  18.243853  27.949503  5.326152\n",
      "10            Sydney  2013-01-10  18.882821  25.307224  4.932224\n",
      "11     SydneyAirport  2013-01-10  17.717533  24.927412  2.695880\n",
      "12        WaggaWagga  2013-01-10  10.348831  29.911538  2.086566\n",
      "13       Williamtown  2013-01-10  19.014979  27.003068  6.074923\n",
      "14        Wollongong  2013-01-10  16.513225  22.739133  4.019463\n",
      "15          Canberra  2013-01-10  13.527491  27.827288  4.337687\n",
      "16       Tuggeranong  2013-01-10  13.496178  27.516369  4.276529\n",
      "17       MountGinini  2013-01-10   6.544297  19.975318  0.738067\n",
      "18          Ballarat  2013-01-10   6.983863  22.752947  1.293806\n",
      "19           Bendigo  2013-01-10   9.212967  27.251526  0.934470\n",
      "20              Sale  2013-01-10  11.031574  25.818815  1.404713\n",
      "21  MelbourneAirport  2013-01-10  10.746100  25.966752  0.779708\n",
      "22         Melbourne  2013-01-10  12.452770  25.312310  2.209962\n",
      "23           Mildura  2013-01-10  11.144879  32.652889  0.219167\n",
      "24          Portland  2013-01-10  11.192444  22.177997  0.817920\n",
      "25          Watsonia  2013-01-10  10.634747  25.821431  1.863703\n",
      "26          Dartmoor  2013-01-10   9.400058  25.095651  0.583980\n",
      "27          Brisbane  2013-01-10  22.295583  31.814481  1.158288\n",
      "28            Cairns  2013-01-10  23.531501  31.751898  7.875951\n",
      "29         GoldCoast  2013-01-10  23.023352  30.677738  1.431016\n",
      "30        Townsville  2013-01-10  24.147724  32.556750  1.575697\n",
      "31          Adelaide  2013-01-10  14.839752  29.426007  0.675066\n",
      "32      MountGambier  2013-01-10  10.013517  24.808064  0.721173\n",
      "33         Nuriootpa  2013-01-10  11.307530  28.090298  0.880565\n",
      "34           Woomera  2013-01-10  15.519019  33.332635  0.573182\n",
      "35            Albany  2013-01-10  17.428507  22.530735  1.925462\n",
      "36       Witchcliffe  2013-01-10  14.602855  25.620110 -2.314646\n",
      "37        PearceRAAF  2013-01-10  17.426645  28.980376  3.370721\n",
      "38      PerthAirport  2013-01-10  17.599777  27.522221  3.653280\n",
      "39             Perth  2013-01-10  17.977489  26.799466  2.904444\n",
      "40        SalmonGums  2013-01-10  21.136158  37.513701  1.154340\n",
      "41           Walpole  2013-01-10  15.664971  23.723717  2.009488\n",
      "42            Hobart  2013-01-10  10.624411  23.539616 -0.175578\n",
      "43        Launceston  2013-01-10   9.933401  24.209013  0.716614\n",
      "44      AliceSprings  2013-01-10  23.964495  42.021196  0.585651\n",
      "45            Darwin  2013-01-10  26.202426  33.537672  6.120278\n"
     ]
    }
   ],
   "source": [
    "def predict_weather_for_all_locations(df, year, month, day, window_size=7):\n",
    "    \"\"\"\n",
    "    Predict MaxTemp, MinTemp, and Rainfall for a specified date across all locations.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with 'Date', 'Location', and columns ['MinTemp', 'MaxTemp', 'Rainfall'].\n",
    "    - year (int): Year of the target prediction date.\n",
    "    - month (int): Month of the target prediction date.\n",
    "    - day (int): Day of the target prediction date.\n",
    "    - window_size (int): Number of past days to use as input features.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing predictions for all locations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    target_date_str = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    \n",
    "    # Iterate over all unique locations in the DataFrame\n",
    "    for location in df['Location'].unique():\n",
    "        try:\n",
    "            # Use the predict_weather_by_location function for each location\n",
    "            prediction = predict_weather_by_location(df, year, month, day, location, window_size)\n",
    "            # Append the results with location and target date info\n",
    "            results.append({\n",
    "                'Location': location,\n",
    "                'Date': target_date_str,\n",
    "                **prediction\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping location '{location}': {e}\")\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "\n",
    "predictions_df = predict_weather_for_all_locations(df, 2013, 1, 10)\n",
    "\n",
    "# Display results\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e5c9823d-e119-45ed-8681-a16c4282d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Geocoding\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in predictions_df['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the dataframe\n",
    "predictions_df.loc[:, 'Longitude'] = lons\n",
    "predictions_df.loc[:, 'Latitude'] = lats\n",
    "predictions_df = predictions_df.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract data\n",
    "lons = predictions_df['Longitude'].values\n",
    "lats = predictions_df['Latitude'].values\n",
    "max_temp = predictions_df['MaxTemp'].values\n",
    "min_temp = predictions_df['MinTemp'].values\n",
    "rainfall = predictions_df['Rainfall'].values\n",
    "\n",
    "# Get the date from the predictions_df\n",
    "forecast_date = predictions_df['Date'].iloc[0]  # Assuming 'Date' column exists and is consistent\n",
    "\n",
    "# Function to create interpolation map\n",
    "def create_interpolation_map(ax, lons, lats, values, title, cmap, cbar_label):\n",
    "    # Define a grid for interpolation\n",
    "    lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "    lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "    lon_grid, lat_grid = np.meshgrid(\n",
    "        np.linspace(lon_min, lon_max, 200),\n",
    "        np.linspace(lat_min, lat_max, 200)\n",
    "    )\n",
    "\n",
    "    # Perform Ordinary Kriging\n",
    "    krige = OrdinaryKriging(lons, lats, values, variogram_model='linear')\n",
    "    grid, _ = krige.execute('grid', lon_grid[0], lat_grid[:, 0])\n",
    "\n",
    "    # Add features to the map\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "    # Plot interpolated values\n",
    "    contour = ax.contourf(\n",
    "        lon_grid, lat_grid, grid,\n",
    "        levels=20, cmap=cmap, transform=ccrs.PlateCarree()\n",
    "    )\n",
    "\n",
    "    # Add data points\n",
    "    ax.scatter(lons, lats, edgecolors='k', s=50, label='Locations', c='white', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contour, ax=ax, orientation='vertical')\n",
    "    cbar.set_label(cbar_label)  # Colorbar only, no black label on map\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create side-by-side maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "create_interpolation_map(axes[0], lons, lats, max_temp, \"Max Temperature (°C)\", 'coolwarm', 'Temperature (°C)')\n",
    "create_interpolation_map(axes[1], lons, lats, min_temp, \"Min Temperature (°C)\", 'viridis', 'Temperature (°C)')\n",
    "create_interpolation_map(axes[2], lons, lats, rainfall, \"Rainfall (mm)\", 'Blues', 'Rainfall (mm)')\n",
    "\n",
    "# Add a descriptive label for the entire figure\n",
    "fig.text(0.5, 0.02, f\"Weather Forecast for Australia on {forecast_date}\", ha='center', fontsize=14)\n",
    "\n",
    "# Adjust layout: Add space between the maps\n",
    "plt.subplots_adjust(wspace=5)  # Increase wspace to add spacing between maps\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0.04, 1, 1])  # Leave space for the bottom label\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2726bf39-ffad-479e-a4a2-dfe916e7f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kliima\n",
    "\n",
    "## kliimadiagramm kus iga line on üks ilmajaam, 10 joont ntx\n",
    "\n",
    "## bar plot kus võrdleme kõige kuivema ja niiskema ilmajaama rainfalli\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "54b651ee-519e-4654-83f1-89549b485f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Predictions vs Actual Weather by Location on 2013-01-10 00:00:00\n",
      "       Location MinTemp_Diff MaxTemp_Diff Rainfall_Diff\n",
      "                        mean         mean          mean\n",
      "0      Adelaide     1.239752    -4.473993      0.675066\n",
      "1        Albany    -0.871493     1.430735      1.925462\n",
      "2        Albury     0.470822    -2.925068      3.294615\n",
      "3      Ballarat    -0.116137    -6.347053      1.293806\n",
      "4       Bendigo     0.212967    -3.248474      0.934470\n",
      "5      Brisbane    -1.204417     0.114481      1.158288\n",
      "6        Cairns     1.031501    -0.348102      7.875951\n",
      "7      Canberra     3.527491    -1.872712      4.337687\n",
      "8         Cobar     0.711503    -3.195215      1.928623\n",
      "9      Dartmoor    -0.699942    -3.104349      0.583980\n",
      "10       Darwin    -1.697574     0.237672      6.120278\n",
      "11    GoldCoast    -0.976648     0.477738      1.431016\n",
      "12       Hobart    -0.375589     1.439616     -0.175578\n",
      "13   Launceston     1.533401     3.909013      0.716614\n",
      "14    Melbourne    -1.747230    -1.887690      2.009962\n",
      "15      Mildura    -0.655121     0.052889      0.219167\n",
      "16        Moree     8.410734    -0.905073      1.017792\n",
      "17    Newcastle     5.606356     0.038580      6.765682\n",
      "18    Nuriootpa     0.607530    -6.509702      0.880565\n",
      "19      Penrith    -0.720968     1.890379      4.879299\n",
      "20        Perth     2.377489     0.599466      2.904444\n",
      "21     Portland     0.992444     1.977997      0.817920\n",
      "22     Richmond    -1.156147     0.649503      5.326152\n",
      "23         Sale     3.931574     2.618815      1.404713\n",
      "24       Sydney    -1.317179    -0.092776      4.932224\n",
      "25   Townsville    -0.252276    -0.443250      1.575697\n",
      "26  Tuggeranong     0.796178    -1.883631      3.876529\n",
      "27      Walpole    -1.035029     0.723717      2.009488\n",
      "28     Watsonia    -2.865253    -4.678569      1.863703\n",
      "29  Williamtown    -1.585021     1.103068      6.074923\n",
      "30  Witchcliffe    -2.497145     1.020110     -2.314646\n",
      "31   Wollongong    -1.286775    -2.860867      4.019463\n",
      "32      Woomera    -0.780981    -3.667365      0.573182\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "\n",
    "#võrdlus reaalsete andmetega ennustuse kuupäeval\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])\n",
    "\n",
    "# Merge the dataframes on 'Date' and 'Location'\n",
    "comparison_df = pd.merge(\n",
    "    predictions_df, \n",
    "    df, \n",
    "    on=['Date', 'Location'], \n",
    "    suffixes=('_predicted', '_actual')\n",
    ")\n",
    "\n",
    "# Calculate differences\n",
    "comparison_df['MinTemp_Diff'] = comparison_df['MinTemp_predicted'] - comparison_df['MinTemp_actual']\n",
    "comparison_df['MaxTemp_Diff'] = comparison_df['MaxTemp_predicted'] - comparison_df['MaxTemp_actual']\n",
    "comparison_df['Rainfall_Diff'] = comparison_df['Rainfall_predicted'] - comparison_df['Rainfall_actual']\n",
    "\n",
    "# Group by location and summarize the differences\n",
    "summary = comparison_df.groupby('Location').agg({\n",
    "    'MinTemp_Diff': ['mean'],\n",
    "    'MaxTemp_Diff': ['mean'],\n",
    "    'Rainfall_Diff': ['mean']\n",
    "})\n",
    "\n",
    "# Reset index for easy viewing\n",
    "summary = summary.reset_index()\n",
    "\n",
    "prediction_date= predictions_df['Date'].iloc[1]\n",
    "# Display the summary\n",
    "print(f\"Comparison of Predictions vs Actual Weather by Location on {prediction_date}\" )\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970904dd-2b00-41bd-92a0-87dbcc6d059c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
