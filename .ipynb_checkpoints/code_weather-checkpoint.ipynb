{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70384ab",
   "metadata": {},
   "source": [
    "# Predicting weather in Australia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9065ed-2e79-4f5a-97a3-cfbd22afe1d9",
   "metadata": {},
   "source": [
    "Made by Mihkel Paal and Laura Heleene Tirkkonen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d3b14",
   "metadata": {},
   "source": [
    "## 1. Importing libraries/data and encoding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c86634d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
      "0           W           44.0          W  ...        71.0         22.0   \n",
      "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
      "2         WSW           46.0          W  ...        38.0         30.0   \n",
      "3          NE           24.0         SE  ...        45.0         16.0   \n",
      "4           W           41.0        ENE  ...        82.0         33.0   \n",
      "\n",
      "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
      "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
      "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
      "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
      "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
      "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
      "\n",
      "   RainTomorrow  \n",
      "0            No  \n",
      "1            No  \n",
      "2            No  \n",
      "3            No  \n",
      "4            No  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "There are 7 categorical variables\n",
      "\n",
      "Categorical variables are : ['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import geopandas as gpd    \n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # data visualization\n",
    "sns.reset_defaults()\n",
    "import geoplot as gplt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"weatherAUS.csv\")\n",
    "print(df.head())\n",
    "df_map = pd.read_csv(\"weatherAUS.csv\")\n",
    "\n",
    "\n",
    "## Find categorical variables\n",
    "\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('Categorical variables are :', categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ab23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find missing values in categorical variables\n",
    "\n",
    "#print(df[categorical].isnull().sum())\n",
    "\n",
    "##frequency of categorical variables\n",
    "\n",
    "#for var in categorical: \n",
    "        \n",
    "#print(df[var].value_counts())\n",
    "\n",
    "##check for cardinality in categorical variables\n",
    "\n",
    "#for var in categorical:\n",
    "    \n",
    "#    print(var, ' contains ', len(df[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17dafde9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Date variable contains 3436 labels so needs to be split into year/month/day\n",
    "\n",
    "#print(df[\"Date\"].dtypes)\n",
    "\n",
    "df['Date']= pd.to_datetime(df['Date'])\n",
    "\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "df.drop('Date', axis=1, inplace = True)\n",
    "\n",
    "#start looking into other categorical variables\n",
    "\n",
    "#print('Location contains', len(df.Location.unique()), 'labels')\n",
    "\n",
    "#print(df.Location.unique())\n",
    "\n",
    "\n",
    "# Add most popular values for missing categorical values\n",
    "\n",
    "for df2 in [df]:\n",
    "    df2['WindGustDir'] = df2['WindGustDir'].fillna(df2['WindGustDir'].mode()[0])\n",
    "    df2['WindDir9am'] = df2['WindDir9am'].fillna(df2['WindDir9am'].mode()[0])\n",
    "    df2['WindDir3pm'] = df2['WindDir3pm'].fillna(df2['WindDir3pm'].mode()[0])\n",
    "    df2['RainToday'] = df2['RainToday'].fillna(df2['RainToday'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6d5e79-e107-436b-ab4c-5c3a96deed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables\n",
    "\n",
    "## location is necessary for maps, so this is omitted as a dummy variable from df_map\n",
    "df_map = pd.get_dummies(df, columns=[ 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'], drop_first=True, dummy_na=True)\n",
    "\n",
    "## For other needs\n",
    "df = pd.get_dummies(df, columns=['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'], drop_first=True, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a541ea72-8513-48a3-bde0-ca36846bbada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTemp            1485\n",
      "MaxTemp            1261\n",
      "Rainfall           3261\n",
      "Evaporation       62790\n",
      "Sunshine          69835\n",
      "                  ...  \n",
      "WindDir3pm_WNW        0\n",
      "WindDir3pm_WSW        0\n",
      "WindDir3pm_nan        0\n",
      "RainToday_Yes         0\n",
      "RainToday_nan         0\n",
      "Length: 118, dtype: int64\n",
      "        MinTemp   MaxTemp  Rainfall  Evaporation  Sunshine  WindGustSpeed  \\\n",
      "count  143975.0  144199.0  142199.0      82670.0   75625.0       135197.0   \n",
      "mean       12.0      23.0       2.0          5.0       8.0           40.0   \n",
      "std         6.0       7.0       8.0          4.0       4.0           14.0   \n",
      "min        -8.0      -5.0       0.0          0.0       0.0            6.0   \n",
      "25%         8.0      18.0       0.0          3.0       5.0           31.0   \n",
      "50%        12.0      23.0       0.0          5.0       8.0           39.0   \n",
      "75%        17.0      28.0       1.0          7.0      11.0           48.0   \n",
      "max        34.0      48.0     371.0        145.0      14.0          135.0   \n",
      "\n",
      "       WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  Pressure9am  \\\n",
      "count      143693.0      142398.0     142806.0     140953.0     130395.0   \n",
      "mean           14.0          19.0         69.0         52.0       1018.0   \n",
      "std             9.0           9.0         19.0         21.0          7.0   \n",
      "min             0.0           0.0          0.0          0.0        980.0   \n",
      "25%             7.0          13.0         57.0         37.0       1013.0   \n",
      "50%            13.0          19.0         70.0         52.0       1018.0   \n",
      "75%            19.0          24.0         83.0         66.0       1022.0   \n",
      "max           130.0          87.0        100.0        100.0       1041.0   \n",
      "\n",
      "       Pressure3pm  Cloud9am  Cloud3pm   Temp9am   Temp3pm      Year  \\\n",
      "count     130432.0   89572.0   86102.0  143693.0  141851.0  145460.0   \n",
      "mean        1015.0       4.0       5.0      17.0      22.0    2013.0   \n",
      "std            7.0       3.0       3.0       6.0       7.0       3.0   \n",
      "min          977.0       0.0       0.0      -7.0      -5.0    2007.0   \n",
      "25%         1010.0       1.0       2.0      12.0      17.0    2011.0   \n",
      "50%         1015.0       5.0       5.0      17.0      21.0    2013.0   \n",
      "75%         1020.0       7.0       7.0      22.0      26.0    2015.0   \n",
      "max         1040.0       9.0       9.0      40.0      47.0    2017.0   \n",
      "\n",
      "          Month       Day  \n",
      "count  145460.0  145460.0  \n",
      "mean        6.0      16.0  \n",
      "std         3.0       9.0  \n",
      "min         1.0       1.0  \n",
      "25%         3.0       8.0  \n",
      "50%         6.0      16.0  \n",
      "75%         9.0      23.0  \n",
      "max        12.0      31.0   2\n"
     ]
    }
   ],
   "source": [
    "# Explore numerical variables\n",
    "\n",
    "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "\n",
    "#print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "#print('The numerical variables are :', numerical)\n",
    "\n",
    "#19 numerical variables, all continuous type\n",
    "#check for missing values\n",
    "\n",
    "print(df[numerical].isnull().sum())\n",
    "\n",
    "print(round(df[numerical].describe()),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e8cb1",
   "metadata": {},
   "source": [
    "## 2. Plotting weather variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b081edfd-1a98-4a63-8b62-6e164469ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boxplots of all weather variables\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = df.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = df.boxplot(column='Evaporation')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Evaporation')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "fig = df.boxplot(column='WindSpeed9am')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed9am')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "fig = df.boxplot(column='WindSpeed3pm')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindSpeed3pm')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ad039",
   "metadata": {},
   "source": [
    "## 3. Prediction of rain tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfa8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert column 'RainTomorrow' to numeric variable\n",
    "\n",
    "df['RainTomorrow'] = df['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "## Handle missing values in column 'RainTomorrow'\n",
    "df = df.dropna(subset=['RainTomorrow'])  # dropping rows with NaN in 'RainTomorrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b248c66-c568-4044-a145-7b24ef654327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataframes with and without target \n",
    "\n",
    "X = df.drop(['RainTomorrow'], axis=1)\n",
    "\n",
    "y = df['RainTomorrow']\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Replace missing values with mean for imputation\n",
    "\n",
    "num_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "num_imputer = SimpleImputer(strategy='mean')  # replacing NaNs with mean for numerical features\n",
    "X[num_cols] = num_imputer.fit_transform(X[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4e343a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Split data into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908613bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7963711804212525\n",
      "[[17632  4380]\n",
      " [ 1411  5016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.80      0.86     22012\n",
      "         1.0       0.53      0.78      0.63      6427\n",
      "\n",
      "    accuracy                           0.80     28439\n",
      "   macro avg       0.73      0.79      0.75     28439\n",
      "weighted avg       0.84      0.80      0.81     28439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Scale data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## Initializing logistic regression model --> as we have binary classification task\n",
    "## Logistic regression assumes linear relationship between variables\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
    "\n",
    "model_logreg = LogisticRegression(class_weight='balanced', C=0.1, solver='lbfgs')\n",
    "model_logreg.fit(X_train, y_train)\n",
    "\n",
    "## Predictions and evaluation\n",
    "y_pred = model_logreg.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca19d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37775417569627595\n",
      "0.8749649857874884\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE \n",
    "\n",
    "y_pred_proba = model_logreg.predict_proba(X_test)[:, 1]  # probability of 'RainTomorrow' being 1\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_proba))\n",
    "print(rmse)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "54de1ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "## Find  best parameters for log. regression model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fd0bf",
   "metadata": {},
   "source": [
    "** The minimum requirement of 80% accuracy which was indicated in project report was achieved "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcc108",
   "metadata": {},
   "source": [
    "## 4. Creating maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96eda74",
   "metadata": {},
   "source": [
    "### 4.1 Example temperature map for April 30th, 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "77c4351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map of temperatures on a certain time (must use df_map)\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Filter data for the specific date\n",
    "df_filtered = df_map[(df_map['Year'] == 2012) & (df_map['Month'] == 4) & (df_map['Day'] == 30)].copy()\n",
    "\n",
    "# Geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# Create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in df_filtered['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the filtered dataframe\n",
    "df_filtered.loc[:, 'Longitude'] = lons\n",
    "df_filtered.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# Drop rows with missing coordinates or temperature data\n",
    "df_filtered = df_filtered.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract columns for plotting\n",
    "lons = df_filtered['Longitude']\n",
    "lats = df_filtered['Latitude']\n",
    "temps = df_filtered['MaxTemp']\n",
    "\n",
    "# Create the map plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# Plot temperature points\n",
    "sc = ax.scatter(lons, lats, c=temps, cmap='coolwarm', edgecolors='k', s=100)\n",
    "\n",
    "# Add temperature labels\n",
    "for idx, row in df_filtered.iterrows():\n",
    "    ax.text(row['Longitude'], row['Latitude'] + 0.2, f\"{row['MaxTemp']}°C\", fontsize=10, ha='center', color='black')  # Temp above the point\n",
    "    ax.text(row['Longitude'], row['Latitude'] - 0.2, row['Location'], fontsize=9, ha='center', color='blue')         # Location below the point\n",
    "\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax, orientation='vertical', label='Max Temperature (°C)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Max Temperature across Australia on 30 April 2012\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38661dc1",
   "metadata": {},
   "source": [
    "### 4.2 Avg. max temp for weather stations (January) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2baab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pykrige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d11e2aad-175f-4962-8893-413937b26327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "krige = OrdinaryKriging(lons, lats, temps, variogram_model='linear')\n",
    "\n",
    "# Filter the dataset for January\n",
    "df_january = df_map[df_map['Month'] == 1].copy()\n",
    "\n",
    "# Drop rows with missing temperature data\n",
    "df_january = df_january.dropna(subset=['MaxTemp'])\n",
    "\n",
    "# group by weather station and calculate the average temperature\n",
    "# Assuming 'Location' represents the weather station\n",
    "avg_temps_january = df_january.groupby('Location')['MaxTemp'].mean().reset_index()\n",
    "\n",
    "# rename columns for clarity\n",
    "avg_temps_january.columns = ['Location', 'MaxTemp']\n",
    "\n",
    "# display the result\n",
    "##print(avg_temps_january)\n",
    "\n",
    "\n",
    "# geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in avg_temps_january['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# add latitude and longitude to the filtered dataframe using .loc\n",
    "avg_temps_january.loc[:, 'Longitude'] = lons\n",
    "avg_temps_january.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# drop rows with NaN coordinates or missing MaxTemp values\n",
    "avg_temps_january = avg_temps_january.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# extract data for interpolation\n",
    "lons = avg_temps_january['Longitude'].values\n",
    "lats = avg_temps_january['Latitude'].values\n",
    "temps = avg_temps_january['MaxTemp'].values\n",
    "\n",
    "# define a grid for interpolation\n",
    "lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "lon_grid, lat_grid = np.meshgrid(\n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "# interpolate temperature values onto the grid\n",
    "temp_grid, _ = krige.execute(\n",
    "    'grid', \n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# plot interpolated temperature as a contour map\n",
    "contour = ax.contourf(\n",
    "    lon_grid, lat_grid, temp_grid, \n",
    "    levels=20, cmap='coolwarm', transform=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "sc = ax.scatter(\n",
    "    lons, lats, color='black', edgecolors='k', s=100, label='Data Points'\n",
    ")\n",
    "\n",
    "# add location labels\n",
    "for idx, row in avg_temps_january.iterrows():\n",
    "    ax.text(\n",
    "        row['Longitude'], row['Latitude'] + 0.5,  # Slightly offset above each point\n",
    "        row['Location'], fontsize=9, ha='center', color='green'\n",
    "    )\n",
    "\n",
    "\n",
    "# add labels and title\n",
    "plt.title(\"Interpolated average max. temperature across Australia in January\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6242de3",
   "metadata": {},
   "source": [
    "### 4.3 Interpolation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3440d797-2ae9-4a61-9c48-498240564198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes on the poster\n",
    "# Interpolation, uses kriging and looks good\n",
    "# use df_map\n",
    "# pip install pykrige\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "krige = OrdinaryKriging(lons, lats, temps, variogram_model='linear')\n",
    "\n",
    "\n",
    "# Filter data for a specific date\n",
    "df_filtered = df_map[(df_map['Year'] == 2015) & (df_map['Month'] == 7) & (df_map['Day'] == 8)].copy()\n",
    "\n",
    "# Geocode the locations to get latitude and longitude\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "\n",
    "# Create lists to store latitudes and longitudes\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in df_filtered['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the filtered dataframe using .loc\n",
    "df_filtered.loc[:, 'Longitude'] = lons\n",
    "df_filtered.loc[:, 'Latitude'] = lats\n",
    "\n",
    "# Drop rows with NaN coordinates or missing MaxTemp values\n",
    "df_filtered = df_filtered.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract data for interpolation\n",
    "lons = df_filtered['Longitude'].values\n",
    "lats = df_filtered['Latitude'].values\n",
    "temps = df_filtered['MaxTemp'].values\n",
    "\n",
    "# Define a grid for interpolation\n",
    "lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "lon_grid, lat_grid = np.meshgrid(\n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "# Interpolate temperature values onto the grid\n",
    "temp_grid, _ = krige.execute(\n",
    "    'grid', \n",
    "    np.linspace(lon_min, lon_max, 200), \n",
    "    np.linspace(lat_min, lat_max, 200)\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a Cartopy map with PlateCarree projection\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# Plot interpolated temperature as a contour map\n",
    "contour = ax.contourf(\n",
    "    lon_grid, lat_grid, temp_grid, \n",
    "    levels=20, cmap='coolwarm', transform=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "# Add the original data points\n",
    "sc = ax.scatter(lons, lats, edgecolors='k', s=100, label='Location')\n",
    "\n",
    "# Add a colorbar for the temperature scale\n",
    "cbar = plt.colorbar(contour, ax=ax, orientation='vertical', label='Max Temperature (°C)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Interpolated Max Temperature across Australia \")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "185c6d90-e45e-44df-bfab-55b03e1bb002",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Year', 'Month', 'Day'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[242], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# merge year month day back together\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_map[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Drop Year, Month, Day columns if not needed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_map \u001b[38;5;241m=\u001b[39m df_map\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Year', 'Month', 'Day'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Merge year month day back together\n",
    "df_map['Date'] = pd.to_datetime(df_map[['Year', 'Month', 'Day']])\n",
    "\n",
    "# Drop Year, Month, Day columns if not needed\n",
    "df_map = df_map.drop(['Year', 'Month', 'Day'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "deadbaa3-c3ad-4284-ac58-e777eb8543a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df_map.select_dtypes(include=['float', 'int']).columns\n",
    "num_imputer = SimpleImputer(strategy='mean')  # Replace NaNs with mean for numerical features\n",
    "df_map[numerical_cols] = num_imputer.fit_transform(df_map[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "83085cc5-5738-4d9b-8e11-5018dacc9dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2008-12-01\n",
       "1        2008-12-02\n",
       "2        2008-12-03\n",
       "3        2008-12-04\n",
       "4        2008-12-05\n",
       "            ...    \n",
       "145455   2017-06-21\n",
       "145456   2017-06-22\n",
       "145457   2017-06-23\n",
       "145458   2017-06-24\n",
       "145459   2017-06-25\n",
       "Name: Date, Length: 145460, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2d8c4ff0-ec86-4a0e-8b37-72fad2e00d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Sydney on 2015-02-10: {'MinTemp': 12.596466306993014, 'MaxTemp': 22.03611002545138, 'Rainfall': 1.3899400695087643}\n",
      "Predictions for Albury on 2015-02-15: {'MinTemp': 7.331554314754396, 'MaxTemp': 16.304999164447917, 'Rainfall': 4.359741996769704}\n"
     ]
    }
   ],
   "source": [
    "#First sliding window prediction, next one ise the more sophisticated one\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def predict_weather_by_location(df, year, month, day, location, window_size=7):\n",
    "    \"\"\"\n",
    "    Predict MaxTemp, MinTemp, and Rainfall for a specified date and location \n",
    "    using the last `window_size` days as input.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with 'Date', 'Location', and columns ['MinTemp', 'MaxTemp', 'Rainfall'].\n",
    "    - year (int): Year of the target prediction date.\n",
    "    - month (int): Month of the target prediction date.\n",
    "    - day (int): Day of the target prediction date.\n",
    "    - location (str): Location for which to predict the weather.\n",
    "    - window_size (int): Number of past days to use as input features.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Predicted values for MaxTemp, MinTemp, and Rainfall.\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist\n",
    "    required_columns = ['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame must contain the columns: {required_columns}\")\n",
    "    \n",
    "    # Ensure 'Date' column is datetime64 dtype\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        raise ValueError(\"Ensure 'Date' column is preprocessed as datetime64 dtype.\")\n",
    "    \n",
    "    # Filter data for the specified location\n",
    "    df_location = df[df['Location'] == location].copy()\n",
    "    if df_location.empty:\n",
    "        raise ValueError(f\"No data found for location '{location}'.\")\n",
    "\n",
    "    # Ensure the target date exists in the filtered data\n",
    "    target_date_str = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    target_date = pd.to_datetime(target_date_str)\n",
    "    if target_date not in df_location['Date'].values:\n",
    "        raise ValueError(f\"Target date {target_date_str} is not in the dataset for location '{location}'.\")\n",
    "\n",
    "    # Sort the data by date\n",
    "    df_location = df_location.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Get the target index and check window size availability\n",
    "    target_index = df_location.index[df_location['Date'] == target_date][0]\n",
    "    start_index = target_index - window_size\n",
    "    if start_index < 0:\n",
    "        raise ValueError(f\"Insufficient data to create sliding window for {target_date_str} at location '{location}'.\")\n",
    "\n",
    "    # Extract sliding window data\n",
    "    feature_columns = ['MinTemp', 'MaxTemp', 'Rainfall']\n",
    "    past_data = df_location[feature_columns].iloc[start_index:target_index].to_numpy()\n",
    "    input_features = past_data.flatten()\n",
    "\n",
    "    # Generate features for training\n",
    "    X = []\n",
    "    y = {col: [] for col in feature_columns}\n",
    "    for i in range(window_size, len(df_location)):\n",
    "        X.append(df_location[feature_columns].iloc[i - window_size:i].to_numpy().flatten())\n",
    "        for col in feature_columns:\n",
    "            y[col].append(df_location[col].iloc[i])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = {col: np.array(vals) for col, vals in y.items()}\n",
    "\n",
    "    # Train models for each target variable\n",
    "    models = {}\n",
    "    for col in feature_columns:\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y[col])\n",
    "        models[col] = model\n",
    "\n",
    "    # Predict the target values\n",
    "    predictions = {}\n",
    "    for col, model in models.items():\n",
    "        predictions[col] = model.predict([input_features])[0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "#df['Date'] = pd.to_datetime(df['Date'])  # Pre-convert 'Date' column to datetime\n",
    "\n",
    "# Predict for Sydney on 10th February 2015\n",
    "predicted_values = predict_weather_by_location(df_map, 2015, 5, 10, 'Sydney')\n",
    "print(f\"Predictions for Sydney on 2015-02-10: {predicted_values}\")\n",
    "\n",
    "# Predict for Melbourne on 15th February 2015\n",
    "predicted_values = predict_weather_by_location(df_map, 2015, 5, 10, 'Albury')\n",
    "print(f\"Predictions for Albury on 2015-02-15: {predicted_values}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "344863a6-c7d0-4d38-b9fc-987ce0f44260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping location 'Nhil': Target date 2013-01-10 is not in the dataset for location 'Nhil'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m predict_weather_for_all_locations(df_map, \u001b[38;5;241m2013\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions_df)\n",
      "Cell \u001b[1;32mIn[277], line 24\u001b[0m, in \u001b[0;36mpredict_weather_for_all_locations\u001b[1;34m(df_map, year, month, day, window_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m location \u001b[38;5;129;01min\u001b[39;00m df_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Use the predict_weather_by_location function for each location\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m predict_weather_by_location(df_map, year, month, day, location, window_size)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m# Append the results with location and target date info\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m: location,\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m: target_date_str,\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprediction\n\u001b[0;32m     30\u001b[0m         })\n",
      "Cell \u001b[1;32mIn[249], line 59\u001b[0m, in \u001b[0;36mpredict_weather_by_location\u001b[1;34m(df, year, month, day, location, window_size)\u001b[0m\n\u001b[0;32m     57\u001b[0m y \u001b[38;5;241m=\u001b[39m {col: [] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m feature_columns}\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(window_size, \u001b[38;5;28mlen\u001b[39m(df_location)):\n\u001b[1;32m---> 59\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(df_location[feature_columns]\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m-\u001b[39m window_size:i]\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m feature_columns:\n\u001b[0;32m     61\u001b[0m         y[col]\u001b[38;5;241m.\u001b[39mappend(df_location[col]\u001b[38;5;241m.\u001b[39miloc[i])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4117\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   4115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 4117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[0;32m   4120\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[0;32m   4121\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[0;32m   4122\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[0;32m   4123\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[0;32m   4124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[0;32m   4125\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mtake_nd(taker, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, new_mgr_locs\u001b[38;5;241m=\u001b[39mmgr_locs)\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sliding window predicion that calculates predictions for every location for speicified date and saves them in df_predictions\n",
    "\n",
    "def predict_weather_for_all_locations(df_map, year, month, day, window_size=7):\n",
    "    \"\"\"\n",
    "    Predict MaxTemp, MinTemp, and Rainfall for a specified date across all locations.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with 'Date', 'Location', and columns ['MinTemp', 'MaxTemp', 'Rainfall'].\n",
    "    - year (int): Year of the target prediction date.\n",
    "    - month (int): Month of the target prediction date.\n",
    "    - day (int): Day of the target prediction date.\n",
    "    - window_size (int): Number of past days to use as input features.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing predictions for all locations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    target_date_str = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    \n",
    "    # Iterate over all unique locations in the DataFrame\n",
    "    for location in df_map['Location'].unique():\n",
    "        try:\n",
    "            # Use the predict_weather_by_location function for each location\n",
    "            prediction = predict_weather_by_location(df_map, year, month, day, location, window_size)\n",
    "            # Append the results with location and target date info\n",
    "            results.append({\n",
    "                'Location': location,\n",
    "                'Date': target_date_str,\n",
    "                **prediction\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping location '{location}': {e}\")\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "\n",
    "predictions_df = predict_weather_for_all_locations(df_map, 2013, 1, 10)\n",
    "\n",
    "# Display results\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9823d-e119-45ed-8681-a16c4282d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interpolated maps of Mintemp, Maxtemp and Rainfall based on the sliding window prediction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Geocoding\n",
    "geolocator = Nominatim(user_agent=\"geo_plotting\")\n",
    "lons = []\n",
    "lats = []\n",
    "\n",
    "for location in predictions_df['Location']:\n",
    "    location_info = geolocator.geocode(location + \", Australia\")\n",
    "    if location_info:\n",
    "        lons.append(location_info.longitude)\n",
    "        lats.append(location_info.latitude)\n",
    "    else:\n",
    "        lons.append(np.nan)\n",
    "        lats.append(np.nan)\n",
    "\n",
    "# Add latitude and longitude to the dataframe\n",
    "predictions_df.loc[:, 'Longitude'] = lons\n",
    "predictions_df.loc[:, 'Latitude'] = lats\n",
    "predictions_df = predictions_df.dropna(subset=['Longitude', 'Latitude', 'MaxTemp'])\n",
    "\n",
    "# Extract data\n",
    "lons = predictions_df['Longitude'].values\n",
    "lats = predictions_df['Latitude'].values\n",
    "max_temp = predictions_df['MaxTemp'].values\n",
    "min_temp = predictions_df['MinTemp'].values\n",
    "rainfall = predictions_df['Rainfall'].values\n",
    "\n",
    "# Get the date from the predictions_df\n",
    "forecast_date = predictions_df['Date'].iloc[0]  # Assuming 'Date' column exists and is consistent\n",
    "\n",
    "# Function to create interpolation map\n",
    "def create_interpolation_map(ax, lons, lats, values, title, cmap, cbar_label):\n",
    "    # Define a grid for interpolation\n",
    "    lon_min, lon_max = lons.min() - 1, lons.max() + 1\n",
    "    lat_min, lat_max = lats.min() - 1, lats.max() + 1\n",
    "    lon_grid, lat_grid = np.meshgrid(\n",
    "        np.linspace(lon_min, lon_max, 200),\n",
    "        np.linspace(lat_min, lat_max, 200)\n",
    "    )\n",
    "\n",
    "    # Perform Ordinary Kriging\n",
    "    krige = OrdinaryKriging(lons, lats, values, variogram_model='linear')\n",
    "    grid, _ = krige.execute('grid', lon_grid[0], lat_grid[:, 0])\n",
    "\n",
    "    # Add features to the map\n",
    "    ax.add_feature(cfeature.COASTLINE, edgecolor='black')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, linestyle=':', edgecolor='gray')\n",
    "\n",
    "    # Plot interpolated values\n",
    "    contour = ax.contourf(\n",
    "        lon_grid, lat_grid, grid,\n",
    "        levels=20, cmap=cmap, transform=ccrs.PlateCarree()\n",
    "    )\n",
    "\n",
    "    # Add data points\n",
    "    ax.scatter(lons, lats, edgecolors='k', s=50, label='Locations', c='white', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contour, ax=ax, orientation='vertical')\n",
    "    cbar.set_label(cbar_label)  # Colorbar only, no black label on map\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create side-by-side maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "create_interpolation_map(axes[0], lons, lats, max_temp, \"Max Temperature (°C)\", 'coolwarm', 'Temperature (°C)')\n",
    "create_interpolation_map(axes[1], lons, lats, min_temp, \"Min Temperature (°C)\", 'viridis', 'Temperature (°C)')\n",
    "create_interpolation_map(axes[2], lons, lats, rainfall, \"Rainfall (mm)\", 'Blues', 'Rainfall (mm)')\n",
    "\n",
    "# Add a descriptive label for the entire figure\n",
    "fig.text(0.5, 0.02, f\"Weather Forecast for Australia on {forecast_date}\", ha='center', fontsize=14)\n",
    "\n",
    "# Adjust layout: Add space between the maps\n",
    "plt.subplots_adjust(wspace=5)  # Increase wspace to add spacing between maps\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0.04, 1, 1])  # Leave space for the bottom label\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554462c",
   "metadata": {},
   "source": [
    "## 5. Climate diagrams (10 year period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53837901",
   "metadata": {},
   "source": [
    "### 5.1 Sydney weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e293eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope of the trend line: 0.0127\n",
      "Intercept of the trend line: 18.2546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Sydney weather station\n",
    "\n",
    "# combining year, month, and day into a single Date column\n",
    "#df_map['Date'] = pd.to_datetime(df_map[['Year', 'Month', 'Day']])\n",
    "\n",
    "# filtering for Sydney weather station and the required time period\n",
    "df_sydney = df_map[(df_map['Location'] == 'Sydney') & (df_map['Date'].dt.year >= 2007) & (df_map['Date'].dt.year <= 2017)]\n",
    "\n",
    "# Sort data by date\n",
    "df_sydney = df_sydney.sort_values(by='Date')\n",
    "\n",
    "# Calculate average temperature\n",
    "df_sydney['AvgTemp'] = (df_sydney['MaxTemp'] + df_sydney['MinTemp']) / 2 \n",
    "df_sydney['YearMonth'] = df_sydney['Date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate monthly data\n",
    "monthly_data = df_sydney.groupby('YearMonth').agg(\n",
    "    MonthlyMaxTemp=('MaxTemp', 'mean'),\n",
    "    MonthlyMinTemp=('MinTemp', 'mean'),\n",
    "    MonthlyAvgTemp=('AvgTemp', 'mean'),\n",
    "    TotalRainfall=('Rainfall', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Convert YearMonth to a datetime for plotting\n",
    "monthly_data['YearMonth'] = monthly_data['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Smooth data using a rolling window\n",
    "monthly_data['SmoothedMaxTemp'] = monthly_data['MonthlyMaxTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedMinTemp'] = monthly_data['MonthlyMinTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedAvgTemp'] = monthly_data['MonthlyAvgTemp'].rolling(window=3, center=True).mean()\n",
    "\n",
    "# Prepare data for linear regression\n",
    "monthly_data['NumericTime'] = np.arange(len(monthly_data))  # Numeric time for regression (e.g., 0, 1, 2,...)\n",
    "X = monthly_data[['NumericTime']]\n",
    "y = monthly_data['MonthlyAvgTemp']\n",
    "\n",
    "# Fit the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Predict the trend line\n",
    "monthly_data['TrendLine'] = reg.predict(X)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot Smoothed MaxTemp, MinTemp, and AvgTemp as line plots, also overlay regression trend line\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMaxTemp'], label='Smoothed max. temperature (°C)', color='red', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMinTemp'], label='Smoothed min. temperature (°C)', color='blue', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedAvgTemp'], label='Smoothed average temperature (°C)', color='green', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['TrendLine'], label='Average temperature trend line', color='purple', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Customize primary Y-axis (temperature)\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(alpha=0.5)\n",
    "\n",
    "# Create a secondary Y-axis for Rainfall\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    monthly_data['YearMonth'], \n",
    "    monthly_data['TotalRainfall'], \n",
    "    color='gray', alpha=0.6, label='Monthly rainfall (mm)', width=20\n",
    ")\n",
    "\n",
    "# Customize secondary Y-axis (rainfall)\n",
    "ax2.set_ylabel('Rainfall (mm)', fontsize=12)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Title and layout adjustments\n",
    "plt.title('Weather trends in Sydney (2007–2017)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print regression results\n",
    "print(f\"Slope of the trend line: {reg.coef_[0]:.4f}\")\n",
    "print(f\"Intercept of the trend line: {reg.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d69879e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driest year: 2017 with 865.80 mm of rainfall\n",
      "Wettest month: 2015-04 with 366.80 mm of rainfall\n",
      "Hottest year: 2017 with an average temperature of 20.76 °C\n",
      "Coldest year: 2008 with an average temperature of 17.71 °C\n",
      "Monthly averages:\n",
      "-------------------------\n",
      "        AvgTemp  Rainfall\n",
      "Date                     \n",
      "1     23.916846  3.127599\n",
      "2     23.533529  4.323922\n",
      "3     22.369355  4.483871\n",
      "4     19.440174  5.194815\n",
      "5     16.632097  2.434194\n",
      "6     14.436441  5.822240\n",
      "7     13.489068  2.853047\n",
      "8     14.535842  2.156272\n",
      "9     17.305730  1.819855\n",
      "10    19.123656  2.164734\n",
      "11    21.144248  2.905347\n",
      "12    22.192211  2.391935\n"
     ]
    }
   ],
   "source": [
    "# Driest year\n",
    "driest_year = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.year)['Rainfall'].sum()\n",
    "    .idxmin()\n",
    ")\n",
    "driest_year_rainfall = df_sydney.groupby(df_sydney['Date'].dt.year)['Rainfall'].sum().min()\n",
    "\n",
    "# Wettest month\n",
    "wettest_month = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.to_period('M'))['Rainfall'].sum()\n",
    "    .idxmax()\n",
    ")\n",
    "wettest_month_rainfall = df_sydney.groupby(df_sydney['Date'].dt.to_period('M'))['Rainfall'].sum().max()\n",
    "\n",
    "# Hottest year\n",
    "hottest_year = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmax()\n",
    ")\n",
    "hottest_year_temp = df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean().max()\n",
    "\n",
    "# Coldest year\n",
    "coldest_year = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmin()\n",
    ")\n",
    "coldest_year_temp = df_sydney.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean().min()\n",
    "\n",
    "# Monthly averages across all years\n",
    "monthly_avg = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.month)[['AvgTemp', 'Rainfall']].mean()\n",
    ")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Driest year: {driest_year} with {driest_year_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Wettest month: {wettest_month} with {wettest_month_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Hottest year: {hottest_year} with an average temperature of {hottest_year_temp:.2f} °C\")\n",
    "print(f\"Coldest year: {coldest_year} with an average temperature of {coldest_year_temp:.2f} °C\")\n",
    "print(\"Monthly averages:\")\n",
    "print('-------------------------')\n",
    "print(monthly_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44be48",
   "metadata": {},
   "source": [
    "## 5.2 Darwin weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0b008b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope of the trend line: 0.0042\n",
      "Intercept of the trend line: 27.6526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## Darwuin weather station\n",
    "\n",
    "# combining year, month, and day into a single Date column\n",
    "#df_map['Date'] = pd.to_datetime(df_map[['Year', 'Month', 'Day']])\n",
    "\n",
    "# filtering for Darwin weather station and the required time period\n",
    "df_darwin = df_map[(df_map['Location'] == 'Darwin') & (df_map['Date'].dt.year >= 2007) & (df_map['Date'].dt.year <= 2017)]\n",
    "\n",
    "# Sort data by date\n",
    "df_darwin = df_darwin.sort_values(by='Date')\n",
    "\n",
    "# Calculate average temperature\n",
    "df_darwin['AvgTemp'] = (df_darwin['MaxTemp'] + df_darwin['MinTemp']) / 2 \n",
    "df_darwin['YearMonth'] = df_darwin['Date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate monthly data\n",
    "monthly_data = df_darwin.groupby('YearMonth').agg(\n",
    "    MonthlyMaxTemp=('MaxTemp', 'mean'),\n",
    "    MonthlyMinTemp=('MinTemp', 'mean'),\n",
    "    MonthlyAvgTemp=('AvgTemp', 'mean'),\n",
    "    TotalRainfall=('Rainfall', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Convert YearMonth to a datetime for plotting\n",
    "monthly_data['YearMonth'] = monthly_data['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Smooth data using a rolling window\n",
    "monthly_data['SmoothedMaxTemp'] = monthly_data['MonthlyMaxTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedMinTemp'] = monthly_data['MonthlyMinTemp'].rolling(window=3, center=True).mean()\n",
    "monthly_data['SmoothedAvgTemp'] = monthly_data['MonthlyAvgTemp'].rolling(window=3, center=True).mean()\n",
    "\n",
    "# Prepare data for linear regression\n",
    "monthly_data['NumericTime'] = np.arange(len(monthly_data))  # Numeric time for regression (e.g., 0, 1, 2,...)\n",
    "X = monthly_data[['NumericTime']]\n",
    "y = monthly_data['MonthlyAvgTemp']\n",
    "\n",
    "# Fit the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Predict the trend line\n",
    "monthly_data['TrendLine'] = reg.predict(X)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot Smoothed MaxTemp, MinTemp, and AvgTemp as line plots, also overlay regression trend line\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMaxTemp'], label='Smoothed max. temperature (°C)', color='red', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedMinTemp'], label='Smoothed min. temperature (°C)', color='blue', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['SmoothedAvgTemp'], label='Smoothed average temperature (°C)', color='green', linewidth=1)\n",
    "ax1.plot(monthly_data['YearMonth'], monthly_data['TrendLine'], label='Average temperature trend line', color='purple', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Customize primary Y-axis (temperature)\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Temperature (°C)', fontsize=12)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(alpha=0.5)\n",
    "\n",
    "# Create a secondary Y-axis for Rainfall\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    monthly_data['YearMonth'], \n",
    "    monthly_data['TotalRainfall'], \n",
    "    color='gray', alpha=0.6, label='Monthly rainfall (mm)', width=20\n",
    ")\n",
    "\n",
    "# Customize secondary Y-axis (rainfall)\n",
    "ax2.set_ylabel('Rainfall (mm)', fontsize=12)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Title and layout adjustments\n",
    "plt.title('Weather trends in Darwin (2007–2017)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print regression results\n",
    "print(f\"Slope of the trend line: {reg.coef_[0]:.4f}\")\n",
    "print(f\"Intercept of the trend line: {reg.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "aafab1f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driest year: 2008 with nan mm of rainfall\n",
      "Wettest month: 2011-02 with 1110.20 mm of rainfall\n",
      "Hottest year: 2016 with an average temperature of nan °C\n",
      "Coldest year: 2011 with an average temperature of 26.91 °C\n",
      "Monthly averages:\n",
      "-------------------------\n",
      "        AvgTemp  Rainfall\n",
      "Date                     \n",
      "1     23.916846  3.127599\n",
      "2     23.533529  4.323922\n",
      "3     22.369355  4.483871\n",
      "4     19.440174  5.194815\n",
      "5     16.632097  2.434194\n",
      "6     14.436441  5.822240\n",
      "7     13.489068  2.853047\n",
      "8     14.535842  2.156272\n",
      "9     17.305730  1.819855\n",
      "10    19.123656  2.164734\n",
      "11    21.144248  2.905347\n",
      "12    22.192211  2.391935\n"
     ]
    }
   ],
   "source": [
    "# Driest year\n",
    "driest_year = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.year)['Rainfall'].sum()\n",
    "    .idxmin()\n",
    ")\n",
    "driest_year_rainfall = df_darwin.groupby(df_sydney['Date'].dt.year)['Rainfall'].sum().min()\n",
    "\n",
    "# Wettest month\n",
    "wettest_month = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.to_period('M'))['Rainfall'].sum()\n",
    "    .idxmax()\n",
    ")\n",
    "wettest_month_rainfall = df_darwin.groupby(df_darwin['Date'].dt.to_period('M'))['Rainfall'].sum().max()\n",
    "\n",
    "# Hottest year\n",
    "hottest_year = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmax()\n",
    ")\n",
    "hottest_year_temp = df_darwin.groupby(df_sydney['Date'].dt.year)['AvgTemp'].mean().max()\n",
    "\n",
    "# Coldest year\n",
    "coldest_year = (\n",
    "    df_darwin.groupby(df_darwin['Date'].dt.year)['AvgTemp'].mean()\n",
    "    .idxmin()\n",
    ")\n",
    "coldest_year_temp = df_darwin.groupby(df_darwin['Date'].dt.year)['AvgTemp'].mean().min()\n",
    "\n",
    "# Monthly averages across all years\n",
    "monthly_avg = (\n",
    "    df_sydney.groupby(df_sydney['Date'].dt.month)[['AvgTemp', 'Rainfall']].mean()\n",
    ")\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Driest year: {driest_year} with {driest_year_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Wettest month: {wettest_month} with {wettest_month_rainfall:.2f} mm of rainfall\")\n",
    "print(f\"Hottest year: {hottest_year} with an average temperature of {hottest_year_temp:.2f} °C\")\n",
    "print(f\"Coldest year: {coldest_year} with an average temperature of {coldest_year_temp:.2f} °C\")\n",
    "print(\"Monthly averages:\")\n",
    "print('-------------------------')\n",
    "print(monthly_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a606c9",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "54b651ee-519e-4654-83f1-89549b485f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Predictions vs Actual Weather by Location on 2013-01-10 00:00:00\n",
      "            Location MinTemp_Diff MaxTemp_Diff Rainfall_Diff\n",
      "                             mean         mean          mean\n",
      "0           Adelaide     1.239752    -4.473993      0.675066\n",
      "1             Albany    -0.871493     1.430735      1.925462\n",
      "2             Albury     0.470822    -2.925068      3.294615\n",
      "3       AliceSprings     2.264495    -0.078804      0.585651\n",
      "4      BadgerysCreek    -0.989195     1.131539      4.815527\n",
      "5           Ballarat    -0.116137    -6.347053      1.293806\n",
      "6            Bendigo     0.212967    -3.248474      0.934470\n",
      "7           Brisbane    -1.204417     0.114481      1.158288\n",
      "8             Cairns     1.031501    -0.348102      7.875951\n",
      "9           Canberra     3.527491    -1.872712      4.337687\n",
      "10             Cobar     0.711503    -3.195215      1.928623\n",
      "11      CoffsHarbour     1.548657     4.030924     -7.798027\n",
      "12          Dartmoor    -0.699942    -3.104349      0.583980\n",
      "13            Darwin    -1.697574     0.237672      6.120278\n",
      "14         GoldCoast    -0.976648     0.477738      1.431016\n",
      "15            Hobart    -0.375589     1.439616     -0.175578\n",
      "16        Launceston     1.533401     3.909013      0.716614\n",
      "17         Melbourne    -1.747230    -1.887690      2.009962\n",
      "18  MelbourneAirport     2.046100    -1.833248      0.779708\n",
      "19           Mildura    -0.655121     0.052889      0.219167\n",
      "20             Moree     8.410734    -0.905073      1.017792\n",
      "21      MountGambier     2.013517    -2.991936      0.721173\n",
      "22       MountGinini     2.544297    -0.224682      0.738067\n",
      "23         Newcastle     5.606356     0.038580      6.765682\n",
      "24         NorahHead    -1.199808    -0.110776      4.815879\n",
      "25     NorfolkIsland    -0.833269    -1.513198      0.288469\n",
      "26         Nuriootpa     0.607530    -6.509702      0.880565\n",
      "27        PearceRAAF     3.026645     0.080376      1.009803\n",
      "28           Penrith    -0.720968     1.890379      4.879299\n",
      "29             Perth     2.377489     0.599466      2.904444\n",
      "30      PerthAirport     4.599777     0.122221      3.653280\n",
      "31          Portland     0.992444     1.977997      0.817920\n",
      "32          Richmond    -1.156147     0.649503      5.326152\n",
      "33              Sale     3.931574     2.618815      1.404713\n",
      "34        SalmonGums     4.536158     8.413701      1.154340\n",
      "35            Sydney    -1.317179    -0.092776      4.932224\n",
      "36     SydneyAirport    -1.982467    -1.172588      2.695880\n",
      "37        Townsville    -0.252276    -0.443250      1.575697\n",
      "38       Tuggeranong     0.796178    -1.883631      3.876529\n",
      "39        WaggaWagga    -0.251169    -2.888462      2.086566\n",
      "40           Walpole    -1.035029     0.723717      2.009488\n",
      "41          Watsonia    -2.865253    -4.678569      1.863703\n",
      "42       Williamtown    -1.585021     1.103068      6.074923\n",
      "43       Witchcliffe    -2.497145     1.020110     -2.314646\n",
      "44        Wollongong    -1.286775    -2.860867      4.019463\n",
      "45           Woomera    -0.780981    -3.667365      0.573182\n"
     ]
    }
   ],
   "source": [
    "# Comparison of the sliding window weather forecast with the actual weather observations on the date\n",
    "\n",
    "df_map['Date'] = pd.to_datetime(df_map['Date'])\n",
    "predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])\n",
    "\n",
    "# Merge the dataframes on 'Date' and 'Location'\n",
    "comparison_df = pd.merge(\n",
    "    predictions_df, \n",
    "    df_map, \n",
    "    on=['Date', 'Location'], \n",
    "    suffixes=('_predicted', '_actual')\n",
    ")\n",
    "\n",
    "# Calculate differences\n",
    "comparison_df['MinTemp_Diff'] = comparison_df['MinTemp_predicted'] - comparison_df['MinTemp_actual']\n",
    "comparison_df['MaxTemp_Diff'] = comparison_df['MaxTemp_predicted'] - comparison_df['MaxTemp_actual']\n",
    "comparison_df['Rainfall_Diff'] = comparison_df['Rainfall_predicted'] - comparison_df['Rainfall_actual']\n",
    "\n",
    "# Group by location and summarize the differences\n",
    "summary = comparison_df.groupby('Location').agg({\n",
    "    'MinTemp_Diff': ['mean'],\n",
    "    'MaxTemp_Diff': ['mean'],\n",
    "    'Rainfall_Diff': ['mean']\n",
    "})\n",
    "\n",
    "# Reset index for easy viewing\n",
    "summary = summary.reset_index()\n",
    "\n",
    "prediction_date= predictions_df['Date'].iloc[1]\n",
    "# Display the summary\n",
    "print(f\"Comparison of Predictions vs Actual Weather by Location on {prediction_date}\" )\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
